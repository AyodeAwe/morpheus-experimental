{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WO1UZU2O8wCv"
   },
   "source": [
    "\n",
    "# URL phishing detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqSTBFBk9Du9"
   },
   "source": [
    "### Table of Contents\n",
    "* Introduction\n",
    "* Dataset\n",
    "* Data Preprocessing\n",
    "* Model Training\n",
    "* Conclusions\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kihTnzx1-Ac0",
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "URL phishing is the fraudulent practice of luring individuals to an imposter website where they will download malicious software or reveal confidential information. \n",
    "#### Example of a URL Phishing Attack\n",
    "One of the most common examples of a URL phishing attack is where a fraudster mimics a known company, sending a bogus email with a message saying “Your account has been disabled. Click here to restore it.” \n",
    "\n",
    "Alarmed users then click the link and unwittingly install malware onto their computer. URL phishing goes even further: the cybercriminal creates a bogus website that is linked within the email. When users click it, they go to a site that looks legitimate, but is actually a trap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvqDLSTB-h_8"
   },
   "source": [
    "## Dataset\n",
    "We gather 500K malicious url from popular and open source dataset and also couple of hundreds URLs from windows os memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJckzNLC9z_P"
   },
   "source": [
    "## Imports installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SE_EPnO394NF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "\u001b[K     |█████████████████▏              | 309.1 MB 109.1 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 578.1 MB 18 kB/s /s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (3.16.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.48.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 91.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (49.6.0.post20210108)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.0.1-py3-none-any.whl (5.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (1.21.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 36.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 58.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 45.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 92.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 34.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (3.10.0.2)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.1 MB 32.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 34.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (21.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.26.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 33.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 32.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Installing collected packages: tensorboard-plugin-wit, tensorboard-data-server, grpcio, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.2.0 astunparse-1.6.3 flatbuffers-2.0.7 gast-0.4.0 google-pasta-0.2.0 grpcio-1.48.1 h5py-3.7.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 opt-einsum-3.3.0 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.3.1-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock>=3.0.8 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tldextract) (3.1.0)\n",
      "Requirement already satisfied: idna in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tldextract) (2.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tldextract) (2.26.0)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests>=2.1.0->tldextract) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests>=2.1.0->tldextract) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests>=2.1.0->tldextract) (1.26.7)\n",
      "Requirement already satisfied: six in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests-file>=1.4->tldextract) (1.16.0)\n",
      "Installing collected packages: requests-file, tldextract\n",
      "Successfully installed requests-file-1.5.1 tldextract-3.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting swifter\n",
      "  Downloading swifter-1.3.4.tar.gz (830 kB)\n",
      "\u001b[K     |████████████████████████████████| 830 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (1.3.3)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (5.8.0)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (2021.9.1)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (4.62.3)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (7.6.5)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (2.0.0)\n",
      "Requirement already satisfied: parso>0.4.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (0.7.1)\n",
      "Requirement already satisfied: bleach>=3.1.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (4.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from bleach>=3.1.1->swifter) (1.16.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.8/site-packages (from bleach>=3.1.1->swifter) (21.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/envs/rapids/lib/python3.8/site-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (5.4.1)\n",
      "Requirement already satisfied: partd>=0.3.10 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (2021.9.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.18 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (1.21.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (3.5.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (1.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (5.1.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (5.1.3)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (7.15.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (5.5.5)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (7.0.5)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (4.4.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pexpect in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (3.0.20)\n",
      "Requirement already satisfied: backcall in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.7.5)\n",
      "Requirement already satisfied: pygments in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (2.10.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.17.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (4.8.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (0.17.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from packaging->bleach>=3.1.1->swifter) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from pandas>=1.0.0->swifter) (2021.1)\n",
      "Requirement already satisfied: locket in /opt/conda/envs/rapids/lib/python3.8/site-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/envs/rapids/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (6.4.4)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.11.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.12.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (20.1.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (3.0.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (6.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (1.8.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.5.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/envs/rapids/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.3)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/envs/rapids/lib/python3.8/site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/rapids/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (2.0.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.5.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.8.4)\n",
      "Requirement already satisfied: testpath in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.5.0)\n",
      "Building wheels for collected packages: swifter\n",
      "  Building wheel for swifter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for swifter: filename=swifter-1.3.4-py3-none-any.whl size=16308 sha256=4fa41587e6b55505e288a746161194180dcf3931fd28a9b4c2f6900966d8f1f2\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/66/b4/921e351e63d88696932279d6163e125727c9da70ed8ca38419\n",
      "Successfully built swifter\n",
      "Installing collected packages: swifter\n",
      "Successfully installed swifter-1.3.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "! pip install tensorflow\n",
    "! pip install tldextract\n",
    "! pip install swifter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jt6Gx60A-fZl"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CV_Vj7oG8pPj"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import os\n",
    "import random\n",
    "import re,unicodedata\n",
    "from string import punctuation\n",
    "\n",
    "import swifter\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwtisThM_FPH"
   },
   "source": [
    "### Fixed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "djKe6oiC8pPl"
   },
   "outputs": [],
   "source": [
    "# Stractural feature of the url\n",
    "ADDITIONAL_FEATURES = ['domain_in_alexa','domain_len','domain_numbers','domain_isalnum','subdomain_len','subdomain_numbers_count',\n",
    "            'subdomain_parts_count','tld_len','tld_parts_count','queries_amount','fragments_amount',\n",
    "            'path_len','path_slash_counts','path_double_slash_counts','brand_in_subdomain','brand_in_path','path_max_len']\n",
    "# Max words in each url\n",
    "MAX_LEN= 500\n",
    "# Number of words in nlp model\n",
    "NLP_TOKENS = 2000\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 150 \n",
    "# Size eof batch\n",
    "BATCH_SIZE = 2000\n",
    "# Size of embedding layer\n",
    "EMBEDDING_DIM = 16\n",
    "# Classes weight\n",
    "CLASS_WEIGHTS = {0: 4000, 1:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYQ5bvIi_Mdc"
   },
   "source": [
    "### Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TWuPxRUf8pPn"
   },
   "outputs": [],
   "source": [
    "# Clean url text\n",
    "def clean(text):   \n",
    "    # strip '\n",
    "    text = text.strip(\"'\")\n",
    "    # convert to lower letters\n",
    "    text = text.lower()  \n",
    "    # remove punctuation marks\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    # remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)   \n",
    "    # strip spaces\n",
    "    text = text.strip(\" \")  \n",
    "    return text\n",
    "# Clean url with remove short and long words\n",
    "def clean_nlp(text):\n",
    "    text = clean(text)\n",
    "    text = ' '.join([x for x in text.split(' ') if x.isnumeric()==False and len(x)>1 and len(x)<21])\n",
    "    return text\n",
    "# Strip ' ' and '\\n'\n",
    "def strip_se(url):\n",
    "    return url.strip(\"'\").strip('\\n')\n",
    "# Add 'http://' for url if needed\n",
    "def add_http(url):\n",
    "    if url.startswith('http'):\n",
    "        return url\n",
    "    return 'http://'+url\n",
    "# Get domain\n",
    "def get_domain(url):\n",
    "    domain = tldextract.extract(url).domain\n",
    "    if domain:\n",
    "        return domain\n",
    "    return ''\n",
    "# Get subdomain\n",
    "def get_subdomain(url):\n",
    "    subdomain = tldextract.extract(url).subdomain\n",
    "    if subdomain:\n",
    "        return subdomain\n",
    "    return ''\n",
    "# Get tld\n",
    "def get_tld(url):\n",
    "    tld = tldextract.extract(url).suffix\n",
    "    if tld:\n",
    "        return tld\n",
    "    return ''\n",
    "# Parse the url\n",
    "def get_url_parsed(url):\n",
    "    url_parsed = urlparse(url)\n",
    "    return url_parsed\n",
    "# Get url's path\n",
    "def get_path(url):\n",
    "    url_parsed = urlparse(url)\n",
    "    return url_parsed.path\n",
    "# Get url len\n",
    "def get_len(s):\n",
    "    return len(s)\n",
    "# Get count of nubers in input\n",
    "def get_count_numbers(s):\n",
    "    return sum(c.isdigit() for c in s)\n",
    "# Check if input is alpha-numeric\n",
    "def get_not_alphanumeric(s):\n",
    "    if s.isalnum() == True:\n",
    "        return 1\n",
    "    return 0\n",
    "# Get count of dots\n",
    "def get_count_parts(s):\n",
    "    return len(s.split('.'))\n",
    "# Get count of queries\n",
    "def get_count_queries(s):\n",
    "    url_parsed_query = urlparse(s).query\n",
    "    if url_parsed_query == '':\n",
    "        return 0\n",
    "    return len(url_parsed_query.split('&'))\n",
    "# Get count of fragments\n",
    "def get_count_fragments(s):\n",
    "    url_parsed_fragment = urlparse(s).fragment\n",
    "    if url_parsed_fragment == '':\n",
    "        return 0\n",
    "    return 1\n",
    "# Get count of slash\n",
    "def get_count_slash(s):\n",
    "    return s.count('/')\n",
    "# Get count of double slash\n",
    "def get_double_slash(s):\n",
    "    return s.count('//')\n",
    "# Get count of upper letters\n",
    "def get_count_upper(s):\n",
    "    return sum(1 for c in s if c.isupper())\n",
    "# Check if brand in subdomain\n",
    "def get_brand_in_subdomain(s):\n",
    "    for brand in ['whatsapp','netflix','dropbox','wetransfer','rakuten','itau','outlook','ebay','facebook','hsbc','linkedin','instagram','google','paypal','dhl','alibaba','bankofamerica','apple','microsoft','skype','amazon','yahoo','wellsfargo','americanexpress']:\n",
    "        if brand in s:\n",
    "            return 1\n",
    "    return 0\n",
    "# Check if brand in path\n",
    "def get_brand_in_path(s):\n",
    "    for brand in ['whatsapp','netflix','dropbox','wetransfer','rakuten','itau','outlook','ebay','facebook','hsbc','linkedin','instagram','google','paypal','dhl','alibaba','bankofamerica','apple','microsoft','skype','amazon','yahoo','wellsfargo','americanexpress']:\n",
    "        if brand in s:\n",
    "            return 1\n",
    "    return 0\n",
    "# Check if domain is in Alexa rank\n",
    "def get_domain_alexa(s):\n",
    "    if s in alexa_rank_1k_domain_unique:\n",
    "        return 2\n",
    "    elif s in alexa_rank_100k_domain_unique:\n",
    "        return 1\n",
    "    return 0\n",
    "# Get max of parts length\n",
    "def get_max_len_path(path_clean):\n",
    "    if path_clean == '':\n",
    "        return 0\n",
    "    path_split = [len(f) for f in path_clean.split()]\n",
    "    return np.max(path_split,0)\n",
    "# Check path empty\n",
    "def check_path_empty(path):\n",
    "    if path.strip(\"/\") == \"\":\n",
    "        return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6Ebx6RxC8pPq"
   },
   "outputs": [],
   "source": [
    "# Calculating the features\n",
    "def create_features(df):\n",
    "    df['domain_in_alexa'] = df['domain'].swifter.apply(get_domain_alexa)\n",
    "    df['domain_len'] = df['domain'].swifter.apply(get_len)\n",
    "    df['domain_numbers'] = df['domain'].swifter.apply(get_count_numbers)\n",
    "    df['domain_isalnum'] = df['domain'].swifter.apply(get_not_alphanumeric)\n",
    "    df['subdomain_len'] = df['subdomain'].swifter.apply(get_len)\n",
    "    df['subdomain_numbers_count'] = df['subdomain'].swifter.apply(get_count_numbers)\n",
    "    df['subdomain_parts_count'] = df['subdomain'].swifter.apply(get_count_parts)\n",
    "    df['tld_len'] = df['tld'].swifter.apply(get_len)\n",
    "    df['tld_parts_count'] = df['tld'].swifter.apply(get_count_parts)\n",
    "    df['url_len'] = df['url'].swifter.apply(get_len)\n",
    "    df['queries_amount'] = df['url'].swifter.apply(get_count_queries)\n",
    "    df['fragments_amount'] = df['url'].swifter.apply(get_count_fragments)\n",
    "    df['path_len'] = df['path'].swifter.apply(get_len)\n",
    "    df['path_slash_counts'] = df['path'].swifter.apply(get_count_slash)\n",
    "    df['path_double_slash_counts'] = df['path'].swifter.apply(get_double_slash)\n",
    "    df['upper_amount'] = df['url'].swifter.apply(get_count_upper)\n",
    "    df['brand_in_subdomain'] = df['subdomain'].swifter.apply(get_brand_in_subdomain)\n",
    "    df['brand_in_path'] = df['path'].swifter.apply(get_brand_in_path)  \n",
    "    url_df['path_clean'] = url_df['path'].swifter.apply(lambda x: clean(x))\n",
    "    url_df['path_max_len'] = url_df['path_clean'].swifter.apply(get_max_len_path)\n",
    "    url_df['path_empty'] = df['path'].swifter.apply(check_path_empty)  \n",
    "    return df\n",
    "# Processing the url - domain, subdomain, tld, path and get URL's features\n",
    "def processing(df):\n",
    "    # strip url\n",
    "    df['url'] = df['url'].apply(strip_se)\n",
    "    # add http\n",
    "    df['url'] = df['url'].apply(add_http)\n",
    "    #df['url'].apply(get_url_parsed)\n",
    "    # get domain\n",
    "    df['domain'] = df['url'].apply(get_domain)\n",
    "    # get sub domain\n",
    "    df['subdomain'] = df['url'].apply(get_subdomain)\n",
    "    # get tld\n",
    "    df['tld'] = df['url'].apply(get_tld)\n",
    "    # get path\n",
    "    df['path'] = df['url'].apply(get_path)\n",
    "    # Create features\n",
    "    df = create_features(df)\n",
    "    return df\n",
    "# Data processing\n",
    "def data_preprocessing(df):\n",
    "    df = processing(df)  \n",
    "    df['url_clean'] = df['url_clean'].apply(lambda x: clean_nlp(x))\n",
    "    df['url_clean'] = df['url_clean'].apply(lambda x: clean_nlp(x))\n",
    "    X = df[['url','url_clean']+ADDITIONAL_FEATURES+['label']]\n",
    "    # Split the data for malicious and benign\n",
    "    X_mal = X[X['label'] == 1]\n",
    "    X_ben = X[X['label'] == 0]\n",
    "    Y_mal = X_mal.pop('label')\n",
    "    Y_ben = X_ben.pop('label')\n",
    "    # Split the data to train and test\n",
    "    X_mal_train, X_mal_test, Y_mal_train, Y_mal_test = train_test_split(X_mal, Y_mal, train_size=0.25)\n",
    "    X_ben_train, X_ben_test, Y_ben_train, Y_ben_test = train_test_split(X_ben, Y_ben, train_size=0.8)\n",
    "    X_train = X_mal_train.append(X_ben_train)\n",
    "    Y_train = Y_mal_train.append(Y_ben_train)\n",
    "    X_test = X_mal_test.append(X_ben_test)\n",
    "    Y_test = Y_mal_test.append(Y_ben_test)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "# Stractural features processing\n",
    "def stractural_processing(X_train, X_test):\n",
    "    # Train and test features dataframe\n",
    "    X_train_features = X_train[ADDITIONAL_FEATURES]\n",
    "    X_test_features = X_test[ADDITIONAL_FEATURES]\n",
    "    \n",
    "    max_dict = {}\n",
    "    min_dict = {}\n",
    "    \n",
    "    # Normalize the features\n",
    "    for feature in X_train_features.columns:\n",
    "        max_dict[feature] = X_train_features[feature].max()\n",
    "        min_dict[feature] = X_train_features[feature].min()\n",
    "        X_test_features[feature] = (X_test_features[feature] - X_train_features[feature].min()) / (X_train_features[feature].max() - X_train_features[feature].min())    \n",
    "        X_train_features[feature] = (X_train_features[feature] - X_train_features[feature].min()) / (X_train_features[feature].max() - X_train_features[feature].min())    \n",
    "    \n",
    "    df_max_min = pd.DataFrame(columns = max_dict.keys())\n",
    "    df_max_min = df_max_min.append(min_dict, ignore_index=True)\n",
    "    df_max_min = df_max_min.append(max_dict, ignore_index=True)\n",
    "    return X_train_features, X_test_features, df_max_min\n",
    "# NLP data processing    \n",
    "def nlp_processing(X_train, X_test):\n",
    "    \n",
    "    # Train and test nlp dataframe\n",
    "    X_train_nlp = X_train['url_clean']\n",
    "    X_test_nlp = X_test['url_clean']\n",
    "    # Convert the words to tokens\n",
    "    tokenizer = Tokenizer(num_words=NLP_TOKENS)\n",
    "    \n",
    "    tokenizer.fit_on_texts(X_train_nlp)\n",
    "    vocab_length = tokenizer.num_words + 1\n",
    "    \n",
    "    X_train_nlp = tokenizer.texts_to_sequences(X_train_nlp)\n",
    "    X_test_nlp = tokenizer.texts_to_sequences(X_test_nlp)\n",
    "    \n",
    "    X_train_nlp = pad_sequences(X_train_nlp, maxlen=MAX_LEN, padding='post')\n",
    "    X_test_nlp = pad_sequences(X_test_nlp, maxlen=MAX_LEN, padding='post')\n",
    "    tokenizer_df = pd.DataFrame()\n",
    "    tokenizer_df['keys'] = list(tokenizer.word_index.keys())[0:NLP_TOKENS]\n",
    "    tokenizer_df['values'] = list(tokenizer.word_index.values())[0:NLP_TOKENS]\n",
    "    return X_train_nlp, X_test_nlp, tokenizer_df, vocab_length\n",
    "# Model training\n",
    "def train_model(X_train_nlp, X_train_features, Y_train):\n",
    "    \n",
    "    # Defining the model\n",
    "    inputA = tf.keras.layers.Input(shape=(X_train_nlp.shape[1],))\n",
    "    inputB = tf.keras.layers.Input(shape=(X_train_features.shape[1],))\n",
    "    # First input will process the url text\n",
    "    x = tf.keras.layers.Embedding(vocab_length, EMBEDDING_DIM, input_length=MAX_LEN)(inputA)\n",
    "    x = tf.keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "    # Second input will process the structural of the url\n",
    "    y = tf.keras.layers.Dense(6, activation=\"relu\")(inputB)\n",
    "    y = tf.keras.Model(inputs=inputB, outputs=y)\n",
    "    # Combine the processing of the text and structural of the url\n",
    "    combined = tf.keras.layers.concatenate([x.output, y.output])\n",
    "    # Apply softmax\n",
    "    z = tf.keras.layers.Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[x.input, y.input], outputs=z)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(x=[X_train_nlp, X_train_features], y=Y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,workers=8 ,use_multiprocessing=True,\n",
    "                        class_weight=CLASS_WEIGHTS)\n",
    "\n",
    "    return model\n",
    "# Model evaluation\n",
    "def model_eval(model, X_test_nlp, X_test_features, Y_test):\n",
    "    # Inferencing the test data\n",
    "    Y_pred = model.predict([X_test_nlp, np.array(X_test_features)])\n",
    "    X_test['pred'] = Y_pred\n",
    "    X_test['label'] = Y_test\n",
    "    # Plotting precision-recall curve \n",
    "    recall = []\n",
    "    precision = []\n",
    "    ratio_malicious_benign = 0.05\n",
    "    flag_pass = False\n",
    "    thr_final = 0\n",
    "    for thr in np.arange(0, 1, 0.01):\n",
    "        FPs = len(X_test[(X_test['pred']>thr) & (X_test['label']==0)])\n",
    "        len_ben = len(X_test[X_test['label']==0])\n",
    "        len_mal = len_ben*ratio_malicious_benign\n",
    "        recall_step = len(X_test[(X_test['pred']>thr) & (X_test['label']==1)])/len(X_test[X_test['label']==1])\n",
    "        recall.append(recall_step)\n",
    "        TPs = len_mal*recall_step\n",
    "        precision.append(TPs/(TPs+FPs))\n",
    "        if TPs/(TPs+FPs) > 0.9 and flag_pass == False:\n",
    "            print('Presicion: {}'.format(TPs/(TPs+FPs)))\n",
    "            print('Recall: {}'.format(recall_step))\n",
    "            print('Threshhold: {}'.format(thr))\n",
    "            thr_final = thr\n",
    "            flag_pass = True\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('URLS model')\n",
    "# Model saving\n",
    "def save_model(df_max_min, tokenizer_df, model):\n",
    "    df_max_min.to_csv('max_min_urls.csv',index=False)\n",
    "    tokenizer_df.to_csv('tokenizer_urls.csv',index=False)\n",
    "    model.save('url_model_keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7PSLuT4p8pPt"
   },
   "outputs": [],
   "source": [
    "# Read Alexa rank domain dataframe\n",
    "alexa_rank = pd.read_csv('../datasets/alexa-top-500k.csv',header=None)\n",
    "alexa_rank.columns = ['index','url']\n",
    "alexa_rank_domain = alexa_rank['url'].apply(get_domain)\n",
    "alexa_rank_1k = alexa_rank_domain.iloc[0:1000]\n",
    "alexa_rank_100k = alexa_rank_domain.iloc[1000:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FrcH9k238pPu"
   },
   "outputs": [],
   "source": [
    "alexa_rank_1k_domain_unique = pd.unique(alexa_rank_1k)\n",
    "alexa_rank_100k_domain_unique = pd.unique(alexa_rank_100k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssvTl6AH__Hd"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lWP4a_q_8pPw"
   },
   "outputs": [],
   "source": [
    "url_df = pd.read_csv(\"../datasets/url_training_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzxusDVWAB-S"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "660e993c29624753891307fb3d5bfcd5",
      "6863894e2be84acf8a9f655bce9245e8",
      "9dee7195d95449a8a642720d87637a9b",
      "044c77517a91471c90d3fc314d1271f5",
      "8539bf3a60154b84a3b24d40505e9970",
      "323b8201e87b406c842b20f297d30747",
      "3ea568401bbf41e1ba59dda500d348ca",
      "0aec2d30d2cc42f597028c3d0100731c",
      "a583b3fa16094622ad29af4801549cfb",
      "193b05768dd64354b999a9f9acc897d7",
      "a0af45ac62bd4225b3f799cde403aa57",
      "d5402b6e32c14d83b990d3ad6b08f2f1",
      "473783fcae9246ab854aec5e1729e783",
      "344d2fa658f74155b8fbf6f79c843970",
      "393b8b59337a453fae668f420b8d7ba7",
      "836c22afd7a44764ae955e26d5463657",
      "b7dd0cde88bc4524bbcd736157efa3f0",
      "cc7498b84cc6417396e7008ace875981",
      "89b4eba41bc64405b8ba4898fd42a841",
      "2ca1e808f6be4bfcbda563ae995696c2",
      "f9311606e81d4a6db335fd7c0a8b2cab"
     ]
    },
    "id": "PlFg9CzE8pP0",
    "outputId": "132d695f-7185-4cde-9094-1ffabfea79fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for url model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc097da795d14a3ba835f0c154bfe24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc3920d777949a89dfca14f7875e523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd615cff2654ac29e9277ed9b4f9dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4f077319074fa5ae2bf09416314b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec061dea2b8d495d9aa79e994b5ff297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc792329b8c4f569921dedb2a4c509a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d31e1362d24cf28675177bda9b7c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2871b619b6974735aae8ab85a1dbe22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051db45e9298463cbde297ea592621c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72db4c05d1a14665bfe5dc4bdcb18b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd91e4c38356418ead5c8e5dcd3395bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9814848c9614b6fa5ef49156c6fbca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c4c56ea6ca4029a7aafe5af7f4972c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd8b0a519174f05ae91e19544051fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4082242595e345fc85dbd57514380bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bf93f04a1142799ed4433d1a5991d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44aaf78ba6b748b49e0e98ca8907b9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b193743b62d145f492a3ec015f407d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b0df3fa1ee406dbca11c4a40fd4f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653302c726eb4989910e78be15bb6453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7645c089eca34dbcbcd41201276b4f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Processing data for url model...\")\n",
    "X_train, Y_train, X_test, Y_test = data_preprocessing(url_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkOU3s7jAFy0"
   },
   "source": [
    "### Processing the structural features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "znUXzL-k8pP2",
    "outputId": "560c7fb2-979b-4d92-b2d7-7d92499212bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating stractural URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-ddbe4d1579ff>:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_features[feature] = (X_test_features[feature] - X_train_features[feature].min()) / (X_train_features[feature].max() - X_train_features[feature].min())\n",
      "<ipython-input-6-ddbe4d1579ff>:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_features[feature] = (X_train_features[feature] - X_train_features[feature].min()) / (X_train_features[feature].max() - X_train_features[feature].min())\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating stractural URL features...\")\n",
    "X_train_features, X_test_features, df_max_min = stractural_processing(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q-MYDn3AOLN"
   },
   "source": [
    "### NLP processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qNzTzTJ-8pP4",
    "outputId": "7cd6bced-be06-4cc4-eda0-cb346329d996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating NLP URL features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating NLP URL features...\")\n",
    "X_train_nlp, X_test_nlp, tokenizer_df, vocab_length = nlp_processing(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ypvf11jAQ2j"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_U0AXPI48pP5",
    "outputId": "1fdeda92-d448-42f6-b795-0d3a0408bef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train URL model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train URL model...\")\n",
    "model = train_model(X_train_nlp, X_train_features, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjLUXysGATnz"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3-LN1CUn8pP6",
    "outputId": "523a8845-f181-4456-e94b-3c51f3dfb501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate URL model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate URL model...\")\n",
    "model_eval(model, X_test_nlp, X_test_features, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYFqtVpaAbVt",
    "tags": []
   },
   "source": [
    "## Conclusions\n",
    "Here we show an example of detecting malicious URLs with high Precision 0.995 and moderate Recall 0.55. This model is based just on the URL: processing the stactural of the URL and words in the URL, because many malicious URLs seem legitimate which means that it's impossible to detect them with preprocessed features, the recall is limited. We can improve the model by adding WHOIS (https://who.is/) and VirusTotal (https://www.virustotal.com/) infromation about the URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F8AzWamCBH3"
   },
   "source": [
    "# References\n",
    "- https://github.com/Antimalweb/URLNet \n",
    "- https://developer.nvidia.com/networking/doca \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "phishurl-appshield-combined-rnn-dnn-20220301.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

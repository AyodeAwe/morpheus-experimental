{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WO1UZU2O8wCv"
   },
   "source": [
    "\n",
    "# URL Phishing Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqSTBFBk9Du9"
   },
   "source": [
    "### Table of Contents\n",
    "* Introduction\n",
    "* Dataset\n",
    "* Data Preprocessing\n",
    "* Model Training\n",
    "* Conclusions\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kihTnzx1-Ac0",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "URL phishing is the fraudulent practice of luring individuals to an imposter website where they will download malicious software or reveal confidential information. \n",
    "#### Example of a URL Phishing Attack\n",
    "One of the most common examples of a URL phishing attack is where a fraudster mimics a known company, sending a bogus email with a message saying “Your account has been disabled. Click here to restore it.” \n",
    "\n",
    "Alarmed users then click the link and unwittingly install malware onto their computer. URL phishing goes even further: the cybercriminal creates a bogus website that is linked within the email. When users click it, they go to a site that looks legitimate, but is actually a trap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvqDLSTB-h_8"
   },
   "source": [
    "## Dataset\n",
    "We gather 500K malicious url from popular and open source dataset and also couple of hundreds URLs from windows os memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJckzNLC9z_P"
   },
   "source": [
    "#### Requirments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "SE_EPnO394NF"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CV_Vj7oG8pPj"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import os\n",
    "import random\n",
    "import re,unicodedata\n",
    "from string import punctuation\n",
    "\n",
    "import swifter\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwtisThM_FPH"
   },
   "source": [
    "### Parameters and variables\n",
    "\n",
    "Here we define a set of global variables and parameters that will be used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "djKe6oiC8pPl"
   },
   "outputs": [],
   "source": [
    "# Structural feature of the url\n",
    "ADDITIONAL_FEATURES = ['domain_in_alexa','domain_len','domain_numbers','domain_isalnum','subdomain_len','subdomain_numbers_count',\n",
    "            'subdomain_parts_count','tld_len','tld_parts_count','queries_amount','fragments_amount',\n",
    "            'path_len','path_slash_counts','path_double_slash_counts','brand_in_subdomain','brand_in_path','path_max_len']\n",
    "# Max words in each url\n",
    "MAX_LEN= 500\n",
    "# Number of words in nlp model\n",
    "NLP_TOKENS = 2000\n",
    "# Number of epochs\n",
    "NUM_EPOCHS = 50 \n",
    "# Size eof batch\n",
    "BATCH_SIZE = 2000\n",
    "# Size of embedding layer\n",
    "EMBEDDING_DIM = 16\n",
    "# Classes weight\n",
    "CLASS_WEIGHTS = {0: 4000, 1:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYQ5bvIi_Mdc"
   },
   "source": [
    "### Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "TWuPxRUf8pPn"
   },
   "outputs": [],
   "source": [
    "# Clean url text\n",
    "def clean(text):   \n",
    "    # strip '\n",
    "    text = text.strip(\"'\")\n",
    "    # convert to lower letters\n",
    "    text = text.lower()  \n",
    "    # remove punctuation marks\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    # remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)   \n",
    "    # strip spaces\n",
    "    text = text.strip(\" \")  \n",
    "    return text\n",
    "# Clean url with remove short and long words\n",
    "def clean_nlp(text):\n",
    "    text = clean(text)\n",
    "    text = ' '.join([x for x in text.split(' ') if x.isnumeric()==False and len(x)>1 and len(x)<21])\n",
    "    return text\n",
    "# Strip ' ' and '\\n'\n",
    "def strip_se(url):\n",
    "    return url.strip(\"'\").strip('\\n')\n",
    "# Add 'http://' for url if needed\n",
    "def add_http(url):\n",
    "    if url.startswith('http'):\n",
    "        return url\n",
    "    return 'http://'+url\n",
    "# Get domain\n",
    "def get_domain(url):\n",
    "    domain = tldextract.extract(url).domain\n",
    "    if domain:\n",
    "        return domain\n",
    "    return ''\n",
    "# Get subdomain\n",
    "def get_subdomain(url):\n",
    "    subdomain = tldextract.extract(url).subdomain\n",
    "    if subdomain:\n",
    "        return subdomain\n",
    "    return ''\n",
    "# Get tld\n",
    "def get_tld(url):\n",
    "    tld = tldextract.extract(url).suffix\n",
    "    if tld:\n",
    "        return tld\n",
    "    return ''\n",
    "# Parse the url\n",
    "def get_url_parsed(url):\n",
    "    url_parsed = urlparse(url)\n",
    "    return url_parsed\n",
    "# Get url's path\n",
    "def get_path(url):\n",
    "    url_parsed = urlparse(url)\n",
    "    return url_parsed.path\n",
    "# Get url len\n",
    "def get_len(s):\n",
    "    return len(s)\n",
    "# Get count of nubers in input\n",
    "def get_count_numbers(s):\n",
    "    return sum(c.isdigit() for c in s)\n",
    "# Check if input is alpha-numeric\n",
    "def get_not_alphanumeric(s):\n",
    "    if s.isalnum() == True:\n",
    "        return 1\n",
    "    return 0\n",
    "# Get count of dots\n",
    "def get_count_parts(s):\n",
    "    return len(s.split('.'))\n",
    "# Get count of queries\n",
    "def get_count_queries(s):\n",
    "    url_parsed_query = urlparse(s).query\n",
    "    if url_parsed_query == '':\n",
    "        return 0\n",
    "    return len(url_parsed_query.split('&'))\n",
    "# Get count of fragments\n",
    "def get_count_fragments(s):\n",
    "    url_parsed_fragment = urlparse(s).fragment\n",
    "    if url_parsed_fragment == '':\n",
    "        return 0\n",
    "    return 1\n",
    "# Get count of slash\n",
    "def get_count_slash(s):\n",
    "    return s.count('/')\n",
    "# Get count of double slash\n",
    "def get_double_slash(s):\n",
    "    return s.count('//')\n",
    "# Get count of upper letters\n",
    "def get_count_upper(s):\n",
    "    return sum(1 for c in s if c.isupper())\n",
    "# Check if brand in subdomain\n",
    "def get_brand_in_subdomain(s):\n",
    "    for brand in ['whatsapp','netflix','dropbox','wetransfer','rakuten','itau','outlook','ebay','facebook','hsbc','linkedin','instagram','google','paypal','dhl','alibaba','bankofamerica','apple','microsoft','skype','amazon','yahoo','wellsfargo','americanexpress']:\n",
    "        if brand in s:\n",
    "            return 1\n",
    "    return 0\n",
    "# Check if brand in path\n",
    "def get_brand_in_path(s):\n",
    "    for brand in ['whatsapp','netflix','dropbox','wetransfer','rakuten','itau','outlook','ebay','facebook','hsbc','linkedin','instagram','google','paypal','dhl','alibaba','bankofamerica','apple','microsoft','skype','amazon','yahoo','wellsfargo','americanexpress']:\n",
    "        if brand in s:\n",
    "            return 1\n",
    "    return 0\n",
    "# Check if domain is in Alexa rank\n",
    "def get_domain_alexa(s):\n",
    "    if s in alexa_rank_1k_domain_unique:\n",
    "        return 2\n",
    "    elif s in alexa_rank_100k_domain_unique:\n",
    "        return 1\n",
    "    return 0\n",
    "# Get max of parts length\n",
    "def get_max_len_path(path_clean):\n",
    "    if path_clean == '':\n",
    "        return 0\n",
    "    path_split = [len(f) for f in path_clean.split()]\n",
    "    return np.max(path_split,0)\n",
    "# Check path empty\n",
    "def check_path_empty(path):\n",
    "    if path.strip(\"/\") == \"\":\n",
    "        return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing and feature engineering\n",
    "\n",
    "Define a set of preprocessing and feature engineering steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6Ebx6RxC8pPq"
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    # Calculating the features\n",
    "    \n",
    "    df['domain_in_alexa'] = df['domain'].swifter.apply(get_domain_alexa)\n",
    "    df['domain_len'] = df['domain'].swifter.apply(get_len)\n",
    "    df['domain_numbers'] = df['domain'].swifter.apply(get_count_numbers)\n",
    "    df['domain_isalnum'] = df['domain'].swifter.apply(get_not_alphanumeric)\n",
    "    df['subdomain_len'] = df['subdomain'].swifter.apply(get_len)\n",
    "    df['subdomain_numbers_count'] = df['subdomain'].swifter.apply(get_count_numbers)\n",
    "    df['subdomain_parts_count'] = df['subdomain'].swifter.apply(get_count_parts)\n",
    "    df['tld_len'] = df['tld'].swifter.apply(get_len)\n",
    "    df['tld_parts_count'] = df['tld'].swifter.apply(get_count_parts)\n",
    "    df['url_len'] = df['url'].swifter.apply(get_len)\n",
    "    df['queries_amount'] = df['url'].swifter.apply(get_count_queries)\n",
    "    df['fragments_amount'] = df['url'].swifter.apply(get_count_fragments)\n",
    "    df['path_len'] = df['path'].swifter.apply(get_len)\n",
    "    df['path_slash_counts'] = df['path'].swifter.apply(get_count_slash)\n",
    "    df['path_double_slash_counts'] = df['path'].swifter.apply(get_double_slash)\n",
    "    df['upper_amount'] = df['url'].swifter.apply(get_count_upper)\n",
    "    df['brand_in_subdomain'] = df['subdomain'].swifter.apply(get_brand_in_subdomain)\n",
    "    df['brand_in_path'] = df['path'].swifter.apply(get_brand_in_path)  \n",
    "    url_df['path_clean'] = url_df['path'].swifter.apply(lambda x: clean(x))\n",
    "    url_df['path_max_len'] = url_df['path_clean'].swifter.apply(get_max_len_path)\n",
    "    url_df['path_empty'] = df['path'].swifter.apply(check_path_empty)  \n",
    "    return df\n",
    "\n",
    "def processing(df):\n",
    "    # Processing the url - domain, subdomain, tld, path and get URL's features\n",
    "    \n",
    "    # strip url\n",
    "    df['url'] = df['url'].apply(strip_se)\n",
    "    # add http\n",
    "    df['url'] = df['url'].apply(add_http)\n",
    "    #df['url'].apply(get_url_parsed)\n",
    "    # get domain\n",
    "    df['domain'] = df['url'].apply(get_domain)\n",
    "    # get sub domain\n",
    "    df['subdomain'] = df['url'].apply(get_subdomain)\n",
    "    # get tld\n",
    "    df['tld'] = df['url'].apply(get_tld)\n",
    "    # get path\n",
    "    df['path'] = df['url'].apply(get_path)\n",
    "    # Create features\n",
    "    df = create_features(df)\n",
    "    return df\n",
    "\n",
    "def data_preprocessing(df):\n",
    "    # Data processing\n",
    "    df = processing(df)  \n",
    "    df['url_clean'] = df['url_clean'].apply(lambda x: clean_nlp(x))\n",
    "    df['url_clean'] = df['url_clean'].apply(lambda x: clean_nlp(x))\n",
    "    X = df[['url','url_clean']+ADDITIONAL_FEATURES+['label']]\n",
    "    # Split the data for malicious and benign\n",
    "    X_mal = X[X['label'] == 1]\n",
    "    X_ben = X[X['label'] == 0]\n",
    "    Y_mal = X_mal.pop('label')\n",
    "    Y_ben = X_ben.pop('label')\n",
    "    # Split the data to train and test\n",
    "    X_mal_train, X_mal_test, Y_mal_train, Y_mal_test = train_test_split(X_mal, Y_mal, train_size=0.25)\n",
    "    X_ben_train, X_ben_test, Y_ben_train, Y_ben_test = train_test_split(X_ben, Y_ben, train_size=0.8)\n",
    "    X_train = X_mal_train.append(X_ben_train)\n",
    "    Y_train = Y_mal_train.append(Y_ben_train)\n",
    "    X_test = X_mal_test.append(X_ben_test)\n",
    "    Y_test = Y_mal_test.append(Y_ben_test)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "# Structural features processing\n",
    "def structural_processing(X_train, X_test):\n",
    "    # Train and test features dataframe\n",
    "    X_train_features = X_train[ADDITIONAL_FEATURES]\n",
    "    X_test_features = X_test[ADDITIONAL_FEATURES]\n",
    "    \n",
    "    max_dict = {}\n",
    "    min_dict = {}\n",
    "    \n",
    "    # Normalize the features\n",
    "    for feature in X_train_features.columns:\n",
    "        max_dict[feature] = X_train_features[feature].max()\n",
    "        min_dict[feature] = X_train_features[feature].min()\n",
    "        X_test_features[feature] = (X_test_features[feature] - X_train_features[feature].min()) / (X_train_features[feature].max() - X_train_features[feature].min())    \n",
    "        X_train_features[feature] = (X_train_features[feature] - X_train_features[feature].min()) / (X_train_features[feature].max() - X_train_features[feature].min())    \n",
    "    \n",
    "    df_max_min = pd.DataFrame(columns = max_dict.keys())\n",
    "    df_max_min = df_max_min.append(min_dict, ignore_index=True)\n",
    "    df_max_min = df_max_min.append(max_dict, ignore_index=True)\n",
    "    return X_train_features, X_test_features, df_max_min\n",
    " \n",
    "def nlp_processing(X_train, X_test):\n",
    "    # NLP data processing   \n",
    "    \n",
    "    # Train and test nlp dataframe\n",
    "    X_train_nlp = X_train['url_clean']\n",
    "    X_test_nlp = X_test['url_clean']\n",
    "    # Convert the words to tokens\n",
    "    tokenizer = Tokenizer(num_words=NLP_TOKENS)\n",
    "    \n",
    "    tokenizer.fit_on_texts(X_train_nlp)\n",
    "    vocab_length = tokenizer.num_words + 1\n",
    "    \n",
    "    X_train_nlp = tokenizer.texts_to_sequences(X_train_nlp)\n",
    "    X_test_nlp = tokenizer.texts_to_sequences(X_test_nlp)\n",
    "    \n",
    "    X_train_nlp = pad_sequences(X_train_nlp, maxlen=MAX_LEN, padding='post')\n",
    "    X_test_nlp = pad_sequences(X_test_nlp, maxlen=MAX_LEN, padding='post')\n",
    "    tokenizer_df = pd.DataFrame()\n",
    "    tokenizer_df['keys'] = list(tokenizer.word_index.keys())[0:NLP_TOKENS]\n",
    "    tokenizer_df['values'] = list(tokenizer.word_index.values())[0:NLP_TOKENS]\n",
    "    return X_train_nlp, X_test_nlp, tokenizer_df, vocab_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train_nlp, X_train_features, Y_train):\n",
    "    \n",
    "    # Defining the model\n",
    "    inputA = tf.keras.layers.Input(shape=(X_train_nlp.shape[1],))\n",
    "    inputB = tf.keras.layers.Input(shape=(X_train_features.shape[1],))\n",
    "    # First input will process the url text\n",
    "    x = tf.keras.layers.Embedding(vocab_length, EMBEDDING_DIM, input_length=MAX_LEN)(inputA)\n",
    "    x = tf.keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "    # Second input will process the structural of the url\n",
    "    y = tf.keras.layers.Dense(6, activation=\"relu\")(inputB)\n",
    "    y = tf.keras.Model(inputs=inputB, outputs=y)\n",
    "    # Combine the processing of the text and structural of the url\n",
    "    combined = tf.keras.layers.concatenate([x.output, y.output])\n",
    "    # Apply softmax\n",
    "    z = tf.keras.layers.Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[x.input, y.input], outputs=z)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(x=[X_train_nlp, X_train_features], y=Y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,workers=8 ,use_multiprocessing=True,\n",
    "                        class_weight=CLASS_WEIGHTS)\n",
    "\n",
    "    return model\n",
    "# Model evaluation\n",
    "def model_eval(model, X_test_nlp, X_test_features, Y_test):\n",
    "    # Inferencing the test data\n",
    "    Y_pred = model.predict([X_test_nlp, np.array(X_test_features)])\n",
    "    X_test['pred'] = Y_pred\n",
    "    X_test['label'] = Y_test\n",
    "    # Plotting precision-recall curve \n",
    "    recall = []\n",
    "    precision = []\n",
    "    ratio_malicious_benign = 0.05\n",
    "    flag_pass = False\n",
    "    thr_final = 0\n",
    "    for thr in np.arange(0, 1, 0.01):\n",
    "        FPs = len(X_test[(X_test['pred']>thr) & (X_test['label']==0)])\n",
    "        len_ben = len(X_test[X_test['label']==0])\n",
    "        len_mal = len_ben*ratio_malicious_benign\n",
    "        recall_step = len(X_test[(X_test['pred']>thr) & (X_test['label']==1)])/len(X_test[X_test['label']==1])\n",
    "        recall.append(recall_step)\n",
    "        TPs = len_mal*recall_step\n",
    "        precision.append(TPs/(TPs+FPs))\n",
    "        if TPs/(TPs+FPs) > 0.9 and flag_pass == False:\n",
    "            print('Presicion: {}'.format(TPs/(TPs+FPs)))\n",
    "            print('Recall: {}'.format(recall_step))\n",
    "            print('Threshhold: {}'.format(thr))\n",
    "            thr_final = thr\n",
    "            flag_pass = True\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('URLS model')\n",
    "# Model saving\n",
    "def save_model(df_max_min, tokenizer_df, model):\n",
    "    df_max_min.to_csv('max_min_urls.csv',index=False)\n",
    "    tokenizer_df.to_csv('tokenizer_urls.csv',index=False)\n",
    "    model.save('url_model_keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training dataset\n",
    "\n",
    "Read Alexa rank domain dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7PSLuT4p8pPt"
   },
   "outputs": [],
   "source": [
    "# Read Alexa rank domain dataframe\n",
    "alexa_rank = pd.read_csv('../datasets/alexa-top-500k.csv',header=None)\n",
    "alexa_rank.columns = ['index','url']\n",
    "alexa_rank_domain = alexa_rank['url'].apply(get_domain)\n",
    "alexa_rank_1k = alexa_rank_domain.iloc[0:1000]\n",
    "alexa_rank_100k = alexa_rank_domain.iloc[1000:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "FrcH9k238pPu"
   },
   "outputs": [],
   "source": [
    "alexa_rank_1k_domain_unique = pd.unique(alexa_rank_1k)\n",
    "alexa_rank_100k_domain_unique = pd.unique(alexa_rank_100k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssvTl6AH__Hd"
   },
   "source": [
    "Read training URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lWP4a_q_8pPw"
   },
   "outputs": [],
   "source": [
    "url_df = pd.read_csv(\"../datasets/url_training_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzxusDVWAB-S"
   },
   "source": [
    "### Data processing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "660e993c29624753891307fb3d5bfcd5",
      "6863894e2be84acf8a9f655bce9245e8",
      "9dee7195d95449a8a642720d87637a9b",
      "044c77517a91471c90d3fc314d1271f5",
      "8539bf3a60154b84a3b24d40505e9970",
      "323b8201e87b406c842b20f297d30747",
      "3ea568401bbf41e1ba59dda500d348ca",
      "0aec2d30d2cc42f597028c3d0100731c",
      "a583b3fa16094622ad29af4801549cfb",
      "193b05768dd64354b999a9f9acc897d7",
      "a0af45ac62bd4225b3f799cde403aa57",
      "d5402b6e32c14d83b990d3ad6b08f2f1",
      "473783fcae9246ab854aec5e1729e783",
      "344d2fa658f74155b8fbf6f79c843970",
      "393b8b59337a453fae668f420b8d7ba7",
      "836c22afd7a44764ae955e26d5463657",
      "b7dd0cde88bc4524bbcd736157efa3f0",
      "cc7498b84cc6417396e7008ace875981",
      "89b4eba41bc64405b8ba4898fd42a841",
      "2ca1e808f6be4bfcbda563ae995696c2",
      "f9311606e81d4a6db335fd7c0a8b2cab"
     ]
    },
    "id": "PlFg9CzE8pP0",
    "outputId": "132d695f-7185-4cde-9094-1ffabfea79fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for url model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6bf57d38614563b2110f596c6302fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0fa116490ac4cfeb1fa51b95b18e2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4597cc5400074ba7a5c335d6add6d27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7a33ae91fe4f30ab1a1aeb01c91973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e55238f34c742cfbce0da00c63cccdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6518cd6c833477b8d8202bc615ab132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b17f22fb0241e882e208eb90fb638a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb7ab19822641e7b0200953f9744092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e581ac1b36db41f6b95f3bcbc9e93ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6208cecbb9d46379921ce1c099fa275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97aee5551d04823b41f0878cbb8e241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8666cdcc2d744b2a55060a2f5e21b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ab7adf34bf4c27a4474d4a374a0d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef85531e5f894b5aa3d3e94119f96d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8994253c5e8741e5a8917d22c4ee5d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689164f20b9a4be9ae0180db99e8999d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbb6b3885204b2182a432abc8c338ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f0d94ce0cb4c1eac42d65732e2c070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e69e374b1748699b1c60de13b57bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64f113bec044327b30ce446d7c979b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb2b026c87d484292f698079ff8f9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/540874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Processing data for url model...\")\n",
    "X_train, Y_train, X_test, Y_test = data_preprocessing(url_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkOU3s7jAFy0"
   },
   "source": [
    "### Processing the structural features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "znUXzL-k8pP2",
    "outputId": "560c7fb2-979b-4d92-b2d7-7d92499212bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating stractural URL features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-9dab54c6162e>:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_features[feature] = (X_test_features[feature] - X_train_features[feature].min()) / (X_train_features[feature].max() - X_train_features[feature].min())\n",
      "<ipython-input-9-9dab54c6162e>:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_features[feature] = (X_train_features[feature] - X_train_features[feature].min()) / (X_train_features[feature].max() - X_train_features[feature].min())\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating stractural URL features...\")\n",
    "X_train_features, X_test_features, df_max_min = structural_processing(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q-MYDn3AOLN"
   },
   "source": [
    "### NLP processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "qNzTzTJ-8pP4",
    "outputId": "7cd6bced-be06-4cc4-eda0-cb346329d996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating NLP URL features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating NLP URL features...\")\n",
    "X_train_nlp, X_test_nlp, tokenizer_df, vocab_length = nlp_processing(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ypvf11jAQ2j"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_U0AXPI48pP5",
    "outputId": "1fdeda92-d448-42f6-b795-0d3a0408bef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train URL model...\n",
      "105/105 [==============================] - 1062s 10s/step - loss: 72.2523 - accuracy: 0.5154\n"
     ]
    }
   ],
   "source": [
    "print(\"Train URL model...\")\n",
    "model = train_model(X_train_nlp, X_train_features, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjLUXysGATnz"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "3-LN1CUn8pP6",
    "outputId": "523a8845-f181-4456-e94b-3c51f3dfb501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate URL model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate URL model...\")\n",
    "model_eval(model, X_test_nlp, X_test_features, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYFqtVpaAbVt",
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "Here we show an example of detecting malicious URLs with high Precision 0.995 and moderate Recall 0.55. This model is based on the URL: processing the stactural of the URL and words in the URL, because many malicious URLs seem legitimate which means that it's impossible to detect them with preprocessed features, then the recall is limited. We can improve the model by adding WHOIS (https://who.is/) and VirusTotal (https://www.virustotal.com/) infromation about the URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F8AzWamCBH3"
   },
   "source": [
    "# References\n",
    "- https://github.com/Antimalweb/URLNet \n",
    "- https://developer.nvidia.com/networking/doca \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "phishurl-appshield-combined-rnn-dnn-20220301.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

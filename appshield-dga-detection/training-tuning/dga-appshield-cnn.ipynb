{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1wxCnfvDjfR",
    "tags": []
   },
   "source": [
    "# DGA Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ViGLx7pDw4a"
   },
   "source": [
    "## Table of Contents\n",
    "* Introduction\n",
    "* Dataset\n",
    "* Data Preprocessing\n",
    "* Binary Model Training and Evaluation\n",
    "* Familieis Model Training and Evaluation\n",
    "* Conclusions\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AnMgLK4D6AL"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Domain generation algorithms (DGA) are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. The large number of potential rendezvous points makes it difficult for law enforcement to effectively shut down botnets, since infected computers will attempt to contact some of these domain names every day to receive updates or commands. The use of public-key cryptography in malware code makes it unfeasible for law enforcement and other actors to mimic commands from the malware controllers as some worms will automatically reject any updates not signed by the malware controllers.\n",
    "\n",
    "In this work we run two variants of DGA models on public dataset of DGA, binary DGA and families DGA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wJonJ9RER7S"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We gathered 3M domains from popular datasets of DGAs.\n",
    "i.e. https://data.netlab.360.com/dga/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvA-XuWf79zN",
    "tags": []
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qtWhJU8N78eq"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-FCq782QCBDf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import swifter\n",
    "import tldextract\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD,RMSprop,Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tldextract\n",
    "\n",
    "# import glob\n",
    "# import swifter\n",
    "\n",
    "# import matplotlib.pyplot as pyplot\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torch.nn as nn\n",
    "# import random\n",
    "# import torch.optim as optim\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-QbjcHdFhiG"
   },
   "source": [
    "### Parameters and variables\n",
    "\n",
    "Here we define set of global parameters and fixed variables for training the two variants of DGA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nKI3cdJ7CBDi"
   },
   "outputs": [],
   "source": [
    "# Binary model params\n",
    "EPOCHS = 1#30\n",
    "BATCH_SIZE = 1000\n",
    "MALICIOUS_RATIO = 0.01\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Families model params\n",
    "TRAIN_BATCH_SIZE = 500\n",
    "TEST_BATCH_SIZE = 500\n",
    "EMB_SIZE = 10\n",
    "EPOCHS_SIAMESE = 1#20\n",
    "LEARNING_RATE_SIAMESE = 1e-3\n",
    "CLASS_WEIGHTS = {0: 100, 1:1}\n",
    "# Triplet loss\n",
    "alpha = 1.5  #value between 0-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afOzj7DVFyyb"
   },
   "source": [
    "### Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4GcBS0clCBDk"
   },
   "outputs": [],
   "source": [
    "# Return URL domain\n",
    "def get_domain(url):\n",
    "    domain = tldextract.extract(url).domain\n",
    "    if domain == 'ddns':\n",
    "        print(url)\n",
    "        urls = url.split('.')\n",
    "        urls_i = urls.index('ddns')\n",
    "        if urls_i == 0:\n",
    "            return 'ddns'\n",
    "        print(urls[urls_i-1])\n",
    "        return urls[urls_i-1]\n",
    "    if domain:\n",
    "        return domain\n",
    "    return ''\n",
    "# Make spaces for character processing\n",
    "def get_domain_space(domain):\n",
    "    try:\n",
    "        return \" \".join(domain)\n",
    "    except:\n",
    "        print(domain)\n",
    "        return \"\"\n",
    "# Split train-test - we are trying change the ratio for small DGA families to test them too \n",
    "def split_train_test_dga(df,ratio=0.8):\n",
    "    df_dga = df[df['label']==1]\n",
    "    df_legit = df[df['label']==0]\n",
    "    X_dga, y_dga = df_dga['domain_1'],df_dga['label']\n",
    "    X_legit, y_legit = df_legit['domain_1'],df_legit['label']\n",
    "    train_dga_i = []\n",
    "    train_ben_i = []\n",
    "    test_dga_i = []\n",
    "    test_ben_i = []\n",
    "    # Make the dga train set to be more equale between families without dominant family\n",
    "    for fam in pd.unique(df_dga['type']):\n",
    "        df_dga_fam = df_dga[df_dga['type']==fam]\n",
    "        # Shuffle the dataframe rows\n",
    "        df_dga_fam = df_dga_fam.sample(frac = 1)\n",
    "        if len(df_dga_fam)>10000:\n",
    "            train_dga_i.extend(df_dga_fam.iloc[0:int(ratio*10000)].index)\n",
    "            test_dga_i.extend(df_dga_fam.iloc[int(ratio*10000):].index)\n",
    "        else:\n",
    "            train_dga_i.extend(df_dga_fam.iloc[0:int(ratio*len(df_dga_fam))].index)\n",
    "            test_dga_i.extend(df_dga_fam.iloc[int(ratio*len(df_dga_fam)):].index)\n",
    "    df_legit = df_legit.sample(frac = 1)\n",
    "    train_ben_i.extend(df_legit.iloc[0:int(ratio*len(df_legit))].index)\n",
    "    test_ben_i.extend(df_legit.iloc[int(ratio*len(df_legit)):].index)\n",
    "    train_dga_i.extend(train_ben_i)\n",
    "    test_dga_i.extend(test_ben_i)\n",
    "    X_train = df['domain_1'][train_dga_i]\n",
    "    y_train = df['label'][train_dga_i]\n",
    "    X_test = df['domain_1'][test_dga_i]\n",
    "    y_test = df['label'][test_dga_i]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "# Create data loader for the neural-net\n",
    "def create_data_loader(data,label):\n",
    "    tensor_data = torch.Tensor(data.astype(int))\n",
    "    tensor_label = torch.Tensor(label)\n",
    "    my_dataset = TensorDataset(tensor_data,tensor_label)\n",
    "    data_loader = DataLoader(my_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    return data_loader\n",
    "# Create batches for Siamese networks\n",
    "def create_batch_offline(indices,batch_size,anc_indices,domain_indices=None):\n",
    "    \"\"\"choose an anchor, a positive and a negative batch.\n",
    "    if domain_indices is given, choose anchor only from the specified domains indices. \"\"\"\n",
    "    x_anchors = np.zeros((batch_size, 75))\n",
    "    x_positives = np.zeros_like(x_anchors)\n",
    "    x_negatives = np.zeros_like(x_anchors)\n",
    "\n",
    "    y = encoded_labels[indices]\n",
    "    anc_indices = np.intersect1d(anc_indices,domain_indices,assume_unique=True)\n",
    "    for i in range(0, batch_size):\n",
    "        anc_idx = np.random.choice(anc_indices) \n",
    "        x_anchor = X_data[anc_idx]\n",
    "        y_anchor = encoded_labels[anc_idx]\n",
    "\n",
    "        indices_for_pos = indices[np.where(y == y_anchor)]  #resulting array alway >=1 (the anchor itself)\n",
    "        pos_idx = np.random.choice(indices_for_pos)\n",
    "        indices_for_neg = indices[np.where(y != y_anchor)] \n",
    "        neg_idx = np.random.choice(indices_for_neg)\n",
    "\n",
    "        x_positive = X_data[pos_idx]\n",
    "        x_negative = X_data[neg_idx] \n",
    "        \n",
    "        x_anchors[i] = x_anchor\n",
    "        x_positives[i] = x_positive\n",
    "        x_negatives[i] = x_negative\n",
    "        \n",
    "    return [x_anchors, x_positives, x_negatives]\n",
    "\n",
    "# Generation of Triplets for Triplet-loss\n",
    "def triplets_generator(**kwargs):\n",
    "    while True:\n",
    "        x = create_batch_offline(**kwargs)\n",
    "        dummy_y = np.zeros((x[0].shape[0], 3 , EMB_SIZE))  #dummy y (never used) the size of the siamese input is required\n",
    "        yield x,dummy_y\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor, positive, negative = y_pred[:,:EMB_SIZE], y_pred[:,EMB_SIZE:2*EMB_SIZE], y_pred[:,2*EMB_SIZE:]\n",
    "    p_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "    n_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "    return tf.reduce_mean(tf.maximum(0., p_dist - n_dist + alpha))        \n",
    "# Calculate the distance from centroid        \n",
    "def argmin_label(row, ref, ref_save):\n",
    "    emb = np.array(row[[0,1,2,3,4,5,6,7,8,9]])\n",
    "    list_dist = [np.sum(np.power(emb-ref_save[key],2)) for key in ref]\n",
    "    arg_m = np.argmin(list_dist)\n",
    "    dist_m = np.min(list_dist)\n",
    "    row['predict_label'] = list(domains)[arg_m]\n",
    "    row['predict_dist'] = dist_m\n",
    "    return row\n",
    "# Processing the binary data\n",
    "def data_preprocessing_binary(df):\n",
    "    # Make spaces between domain chars\n",
    "    df['domain_1'] = df['domain'].apply(get_domain_space)\n",
    "    # Split train-test\n",
    "    X_train, X_test, y_train, y_test = split_train_test_dga(df,0.8)\n",
    "\n",
    "    domain_test = df['domain'].iloc[X_test.index]\n",
    "    type_test = df['type'].iloc[X_test.index]\n",
    "    # Convert text to tokens\n",
    "    X_train_np = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train_np = pad_sequences(X_train_np, maxlen=75, padding='post')\n",
    "    X_test_np = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test_np = pad_sequences(X_test_np, maxlen=75, padding='post')\n",
    "    \n",
    "    X_train = np.array(X_train_np).astype(int)\n",
    "    X_test = np.array(X_test_np).astype(int)\n",
    "    \n",
    "    train_loader = create_data_loader(X_train, list(y_train))\n",
    "    test_loader = create_data_loader(X_test, list(y_test))\n",
    "            \n",
    "    return X_train, y_train, X_test, y_test, domain_test, type_test\n",
    "# Train the binary model\n",
    "def train_model_binary(X_train, y_train, X_test, y_test):\n",
    "    # Defining the model\n",
    "    inputA = tf.keras.layers.Input(shape=(X_train.shape[1],), name='input')\n",
    "    x = tf.keras.layers.Embedding(max_features, 128, input_length=75)(inputA)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "    x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "    model = tf.keras.Model(inputs=x.input, outputs=x.output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #batch_size = 1000\n",
    "    model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "             validation_data=([X_test], y_test), class_weight=CLASS_WEIGHTS)\n",
    "    return model\n",
    "# Evaluate the binary model\n",
    "def model_eval_binary(model, X_test, y_test, domain_test, type_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    D_test = pd.DataFrame()\n",
    "    \n",
    "    D_test[\"domain\"] = domain_test\n",
    "    D_test[\"type\"] = type_test\n",
    "    D_test[\"label\"] = y_test\n",
    "    D_test[\"pred\"] = y_pred\n",
    "    \n",
    "    recall = []\n",
    "    precision = []\n",
    "    ratio_malicious_benign = 0.01\n",
    "    flag_pass = False\n",
    "    thr_final = 0\n",
    "    for thr in np.arange(0, 1, 0.01):\n",
    "        FPs = len(D_test[(D_test['pred']>thr) & (D_test['label']==0)])\n",
    "        len_ben = len(D_test[D_test['label']==0])\n",
    "        len_mal = len(D_test[D_test['label']==0])*ratio_malicious_benign\n",
    "        recall_step = len(D_test[(D_test['pred']>thr) & (D_test['label']==1)])/len(D_test[D_test['label']==1])\n",
    "        recall.append(recall_step)\n",
    "        TPs = len_mal*recall_step\n",
    "        precision.append(TPs/(TPs+FPs))\n",
    "        if TPs/(TPs+FPs) > 0.9 and flag_pass == False:\n",
    "            print('Precision: {}'.format(TPs/(TPs+FPs)))\n",
    "            print('Recall: {}'.format(recall_step))\n",
    "            print('Threshhold: {}'.format(thr))\n",
    "            thr_final = thr\n",
    "            flag_pass = True\n",
    "    pyplot.plot(recall, precision, marker='.', label='CNN Pytorch')\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    pyplot.title('DGA model')\n",
    "       \n",
    "    D_test_mal = pd.DataFrame(D_test[D_test['label']==1].groupby(['type'], as_index=False)['label'].sum())\n",
    "    D_test_mal_detected = pd.DataFrame(D_test[(D_test['label']==1) & (D_test['pred']>thr_final)].groupby(['type'], as_index=False)['label'].sum())\n",
    "    D_test_mal_detected.columns = ['type','detected']\n",
    "    D_test_mal = pd.merge(D_test_mal, D_test_mal_detected,how = \"left\", on=[\"type\"])\n",
    "    D_test_mal['detected'] = D_test_mal['detected'].fillna(0)\n",
    "    D_test_mal['ratio'] = D_test_mal['detected']/D_test_mal['label']\n",
    "    print(D_test_mal[(D_test_mal['ratio']<thr_final) & (D_test_mal['label']>D_test_mal['label'].median())])\n",
    "    print(D_test_mal[(D_test_mal['ratio']>thr_final) & (D_test_mal['label']>D_test_mal['label'].median())])\n",
    "\n",
    "# Processing the families data\n",
    "def data_preprocessing_families(df):    \n",
    "    # Merge dgas families with the same pattern\n",
    "    df['type'] = df['type'].replace('FluBot_dga','flubot')\n",
    "    df['type'] = df['type'].replace('fobber_v2','fobber')\n",
    "    df['type'] = df['type'].replace('legit','alexa')\n",
    "    df['type'] = df['type'].replace('pykspa_v2_real','pykspa')\n",
    "    df['type'] = df['type'].replace('pykspa_v2_fake','pykspa')\n",
    "    df['type'] = df['type'].replace('gameoverdga','gameover')\n",
    "    # Merge others dgas families to 1 label\n",
    "    for type_dga in pd.unique(df['type']):\n",
    "        if type_dga not in['goz','bazarbackdoor','bamital','gspy','dyre','enviserv','chinad','monerodownloader','emotet','ramdo','padcrypt','qadars','banjori','corebot','rovnix','flubot','gameover','alexa']:\n",
    "            df['type'] = df['type'].replace(type_dga,'alexa')\n",
    "    \n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    labels_type = df['type']\n",
    "    # Label encoding the dga families\n",
    "    le = LabelEncoder()\n",
    "    encoded_labels = le.fit_transform(labels_type)\n",
    "    dict_type_count = df['type'].value_counts().to_dict()\n",
    "    dict_type_count.pop('alexa')\n",
    "    df['domain_1'] = df['domain'].apply(get_domain_space)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_train_test_dga(df,0.8)\n",
    "       \n",
    "    # Convert text to tokens\n",
    "    X_data = tokenizer.texts_to_sequences(df['domain_1'])\n",
    "    X_data = pad_sequences(X_data, maxlen=75, padding='post')\n",
    "    domains = dict_type_count.keys()\n",
    "    df['new_col'] = df['type'].isin(domains).astype(int)\n",
    "    domains_idx = np.array(df.index[df['new_col'] == 1])\n",
    "    noise_idx = np.array(df.index[df['new_col'] == 0])\n",
    "    indices = domains_idx\n",
    "    train_indices_same = np.intersect1d(X_train.index, domains_idx, assume_unique=False)\n",
    "    train_indices_diff = np.intersect1d(X_train.index, noise_idx, assume_unique=False)\n",
    "    test_indices_same = np.intersect1d(X_test.index, domains_idx, assume_unique=False)\n",
    "    test_indices_diff = np.intersect1d(X_test.index, noise_idx, assume_unique=False) \n",
    "    train_classes, train_cnt = np.unique(encoded_labels[train_indices_same], return_counts=True)\n",
    "    test_classes, test_cnt = np.unique(encoded_labels[test_indices_same], return_counts=True)\n",
    "    stacked = np.stack((train_cnt,test_cnt),axis=1)\n",
    "    \n",
    "    anc_idx = np.random.choice(train_indices_same) \n",
    "    anchor = X_data[anc_idx]\n",
    "    encoded_labels_train = encoded_labels[domains_idx]\n",
    "    steps_per_epoch = int(train_indices_same.size/TRAIN_BATCH_SIZE)\n",
    "    validation_steps = int(test_indices_same.size/TEST_BATCH_SIZE)\n",
    "\n",
    "    train_generator = triplets_generator(indices=X_train.index,batch_size=TRAIN_BATCH_SIZE,anc_indices=train_indices_same,domain_indices=train_indices_same)\n",
    "    validation_generator = triplets_generator(indices=X_test.index,batch_size=TEST_BATCH_SIZE,anc_indices=test_indices_same,domain_indices=test_indices_same)\n",
    "    \n",
    "    return train_generator, validation_generator, X_data, encoded_labels, steps_per_epoch, domains, labels_type, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff\n",
    "\n",
    "# Train the families model\n",
    "def train_model_families(train_generator, steps_per_epoch):  \n",
    "    # Defining the model\n",
    "    inputA = tf.keras.layers.Input(shape=(75,), name='input')\n",
    "    x = tf.keras.layers.Embedding(max_features, 128, input_length=75)(inputA)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(EMB_SIZE, activation=None)(x)\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x,axis=1), name='output')(x)\n",
    "    x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "    model = tf.keras.Model(inputs=x.input, outputs=x.output)\n",
    "    \n",
    "    input_anchor = tf.keras.layers.Input(shape=(75))\n",
    "    input_positive = tf.keras.layers.Input(shape=(75))\n",
    "    input_negative = tf.keras.layers.Input(shape=(75))\n",
    "\n",
    "    embedding_anchor = model(input_anchor)\n",
    "    embedding_positive = model(input_positive)\n",
    "    embedding_negative = model(input_negative)\n",
    "\n",
    "    output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
    "\n",
    "    siamese_net = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)\n",
    "    \n",
    "    siamese_net.compile(loss=triplet_loss, optimizer=Adam(learning_rate=LEARNING_RATE_SIAMESE))\n",
    "    \n",
    "    history = siamese_net.fit(\n",
    "    train_generator, steps_per_epoch=steps_per_epoch, epochs=EPOCHS_SIAMESE, workers=8 ,use_multiprocessing=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluate families model\n",
    "def model_eval_families(model, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff):\n",
    "    train_indices_same = np.sort(train_indices_same)\n",
    "    train_indices_diff = np.sort(train_indices_diff)\n",
    "    x_train_same = X_data[train_indices_same]\n",
    "    x_train_diff = X_data[train_indices_diff]\n",
    "    y_train_same = labels_type.iloc[train_indices_same]\n",
    "    y_train_diff = labels_type.iloc[train_indices_diff]\n",
    "    x_test_same = X_data[test_indices_same]\n",
    "    x_test_diff = X_data[test_indices_diff]\n",
    "    y_test_same = labels_type.iloc[test_indices_same]\n",
    "    y_test_diff = labels_type.iloc[test_indices_diff]\n",
    "        \n",
    "    x_train_same_emb = model.predict(x_train_same)\n",
    "    x_train_diff_emb = model.predict(x_train_diff)\n",
    "    \n",
    "    x_test_same_emb = model.predict(x_test_same)\n",
    "    x_test_diff_emb = model.predict(x_test_diff)\n",
    "    \n",
    "    # Create a dict of the output model\n",
    "    #create a dict of vector embeddings per class:\n",
    "    ref ={}\n",
    "    for domain in domains:\n",
    "        x_domain = x_train_same[np.where(y_train_same == domain)[0]]\n",
    "        ref[domain] = model(x_domain)\n",
    "    ref_save = {}\n",
    "    # Create dict of anchors\n",
    "    for key in ref:\n",
    "        ref_save[key] = ref[key][0]\n",
    "    ref_save_df = pd.DataFrame()\n",
    "    ref_save_df['Family'] = ref_save.keys()\n",
    "    for i in range(len(ref_save['emotet'])):\n",
    "        list_vec = []\n",
    "        for key in ref_save:\n",
    "            list_vec.append(ref_save[key][i].numpy())\n",
    "        ref_save_df[i] = list_vec\n",
    "    \n",
    "    y_test_same_list = y_test_same.tolist()\n",
    "    df_test_same_emb = pd.DataFrame(x_test_same_emb)\n",
    "    df_test_same_emb['label'] = y_test_same_list\n",
    "    df_test_same_emb_mini = df_test_same_emb\n",
    "    df_test_same_emb_mini = df_test_same_emb_mini.swifter.apply(lambda row : argmin_label(row, ref, ref_save),axis=1)\n",
    "    \n",
    "    print(len(df_test_same_emb_mini[(df_test_same_emb_mini['predict_label']==df_test_same_emb_mini['label']) & (df_test_same_emb_mini['predict_dist']<0.5)])/len(df_test_same_emb_mini))\n",
    "    \n",
    "    y_test_diff_list = y_test_diff.tolist()\n",
    "    df_test_diff_emb = pd.DataFrame(x_test_diff_emb)\n",
    "    df_test_diff_emb['label'] = y_test_diff_list\n",
    "    df_test_diff_emb_mini = df_test_diff_emb\n",
    "    df_test_diff_emb_mini = df_test_diff_emb_mini.swifter.apply(lambda row : argmin_label(row, ref, ref_save),axis=1)\n",
    "    \n",
    "    print(len(df_test_diff_emb_mini[(df_test_diff_emb_mini['predict_dist']<0.5) & ~df_test_diff_emb_mini['predict_label'].isin(['simda','fobber','pykspa_v1'])])/len(df_test_same_emb_mini[(~df_test_same_emb_mini['label'].isin(['simda','fobber','pykspa_v1'])) & (df_test_same_emb_mini['predict_label']==df_test_same_emb_mini['label']) & (df_test_same_emb_mini['predict_dist']<0.5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaOrtbm-F42x"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NF4MTVA0CBD2",
    "outputId": "c57736bb-420c-4456-d7a7-d31dd587b233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tokenizer...\n",
      "Reading data for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.word_index = pd.read_csv('../../../morpheus-experimental/appshield-dga-detection/models/tokenizer.csv').set_index('keys')['values'].to_dict()\n",
    "max_features = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Reading tokenizer...\")\n",
    "# Read tokenizer\n",
    "print(\"Reading data for training\")\n",
    "df_binary = pd.read_csv(\"../../../morpheus-experimental/appshield-dga-detection/datasets/dga_training_dataset.csv\")\n",
    "df_families = df_binary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9DuHUfhGOX6"
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5IYjShJLCBD7",
    "outputId": "2c98434e-725a-4f91-fb2c-5566cf8b93b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for binary model...\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing data for binary model...\")\n",
    "X_train, y_train, X_test, y_test, domain_test, type_test = data_preprocessing_binary(df_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3s7-d2DG1JA"
   },
   "source": [
    "# Binary model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "M0_v2MkkCBD9",
    "outputId": "94fb6425-fa87-440b-f1b6-6d9540fca79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training binary model...\n",
      "997/997 [==============================] - 116s 115ms/step - loss: 2.1266 - accuracy: 0.7175 - val_loss: 1.9418 - val_accuracy: 0.1049\n",
      "Evaluating binary model...\n",
      "53301/53301 [==============================] - 199s 4ms/step\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-25c656a995ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_binary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating binary model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_eval_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6ad8984f8014>\u001b[0m in \u001b[0;36mmodel_eval_binary\u001b[0;34m(model, X_test, y_test, domain_test, type_test)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mrecall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mTPs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_mal\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTPs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTPs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFPs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mTPs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTPs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFPs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.9\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mflag_pass\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTPs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTPs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFPs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "print(\"Training binary model...\")\n",
    "model_binary = train_model_binary(X_train, y_train, X_test, y_test)\n",
    "print(\"Evaluating binary model...\")\n",
    "model_eval_binary(model_binary, X_test, y_test, domain_test, type_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_tjDG4WHARC"
   },
   "source": [
    "# Training to detect to which of the DGA is belogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "IZOzqHqYCBEA",
    "outputId": "1ec320aa-36cc-4a5b-89b5-60e781aa5bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for families model...\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing data for families model...\")\n",
    "train_generator, validation_generator, X_data, encoded_labels, steps_per_epoch, domains, labels_type, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff = data_preprocessing_families(df_families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training families model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training families model...\")\n",
    "model_families = train_model_families(train_generator, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clbkRRkYHtua",
    "tags": []
   },
   "source": [
    "# Evaluation of the families model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "2db5860e920e41d1b3780f8be7e63636",
      "765dcced9b6d48b8b7052f338c8cd046"
     ]
    },
    "id": "iKAX7GVKCBEB",
    "outputId": "05f29b1c-37c9-4493-c505-ca5361d59b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating families model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating families model...\")\n",
    "model_eval_families(model_families, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1S_3hCu6QPl"
   },
   "source": [
    "# Conclusions\n",
    "In this work we show an example of detecting character based DGA algorithms with high Precision 0.9, Recall 0.9 and accuracy 0.95 of detecting many character DGA families. In future work, we plan to develop a word-based DGA model, that could tackle the limitation of character based DGA algorithm. Our model is tested on AppShield - BlueField, an agentless system. By using AppShield we succeeded to detect DGA malwares before the malware connects with the command and control server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bf5AZ5Pl7iEm"
   },
   "source": [
    "# References\n",
    "- https://data.netlab.360.com/dga/\n",
    "- https://underdefense.com/guides/detecting-dga-domains-machine-learning-approach/\n",
    "- https://developer.nvidia.com/networking/doca \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dga-appshield-cnn-20220214.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1wxCnfvDjfR",
    "tags": []
   },
   "source": [
    "# DGA Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ViGLx7pDw4a"
   },
   "source": [
    "## Table of Contents\n",
    "* Introduction\n",
    "* Dataset\n",
    "* Data Preprocessing\n",
    "* Binary Model Training and Evaluation\n",
    "* Familieis Model Training and Evaluation\n",
    "* Conclusions\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AnMgLK4D6AL"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Domain generation algorithms (DGA) are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. The large number of potential rendezvous points makes it difficult for law enforcement to effectively shut down botnets, since infected computers will attempt to contact some of these domain names every day to receive updates or commands. The use of public-key cryptography in malware code makes it unfeasible for law enforcement and other actors to mimic commands from the malware controllers as some worms will automatically reject any updates not signed by the malware controllers.\n",
    "\n",
    "In this work we run two variants of DGA models on public dataset of DGA, binary DGA and families DGA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wJonJ9RER7S"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We gathered 3M domains from popular datasets of DGAs.\n",
    "i.e. https://data.netlab.360.com/dga/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvA-XuWf79zN",
    "tags": []
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qtWhJU8N78eq"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-FCq782QCBDf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import swifter\n",
    "import tldextract\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD,RMSprop,Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-QbjcHdFhiG"
   },
   "source": [
    "### Parameters and variables\n",
    "\n",
    "Here we define set of global parameters and fixed variables for training the two variants of DGA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nKI3cdJ7CBDi"
   },
   "outputs": [],
   "source": [
    "# Binary model params\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 1000\n",
    "MALICIOUS_RATIO = 0.01\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Families model params\n",
    "TRAIN_BATCH_SIZE = 500\n",
    "TEST_BATCH_SIZE = 500\n",
    "EMB_SIZE = 10\n",
    "EPOCHS_SIAMESE = 120\n",
    "LEARNING_RATE_SIAMESE = 1e-3\n",
    "CLASS_WEIGHTS = {0: 100, 1:1}\n",
    "# Triplet loss\n",
    "alpha = 1.5  #value between 0-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afOzj7DVFyyb"
   },
   "source": [
    "### Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4GcBS0clCBDk"
   },
   "outputs": [],
   "source": [
    "# Return URL domain\n",
    "def get_domain(url):\n",
    "    domain = tldextract.extract(url).domain\n",
    "    if domain == 'ddns':\n",
    "        print(url)\n",
    "        urls = url.split('.')\n",
    "        urls_i = urls.index('ddns')\n",
    "        if urls_i == 0:\n",
    "            return 'ddns'\n",
    "        print(urls[urls_i-1])\n",
    "        return urls[urls_i-1]\n",
    "    if domain:\n",
    "        return domain\n",
    "    return ''\n",
    "# Make spaces for character processing\n",
    "def get_domain_space(domain):\n",
    "    try:\n",
    "        return \" \".join(domain)\n",
    "    except:\n",
    "        print(domain)\n",
    "        return \"\"\n",
    "# Split train-test - we are trying change the ratio for small DGA families to test them too \n",
    "def split_train_test_dga(df,ratio=0.8):\n",
    "    df_dga = df[df['label']==1]\n",
    "    df_legit = df[df['label']==0]\n",
    "    X_dga, y_dga = df_dga['domain_1'],df_dga['label']\n",
    "    X_legit, y_legit = df_legit['domain_1'],df_legit['label']\n",
    "    train_dga_i = []\n",
    "    train_ben_i = []\n",
    "    test_dga_i = []\n",
    "    test_ben_i = []\n",
    "    # Make the dga train set to be more equale between families without dominant family\n",
    "    for fam in pd.unique(df_dga['type']):\n",
    "        df_dga_fam = df_dga[df_dga['type']==fam]\n",
    "        # Shuffle the dataframe rows\n",
    "        df_dga_fam = df_dga_fam.sample(frac = 1)\n",
    "        if len(df_dga_fam)>10000:\n",
    "            train_dga_i.extend(df_dga_fam.iloc[0:int(ratio*10000)].index)\n",
    "            test_dga_i.extend(df_dga_fam.iloc[int(ratio*10000):].index)\n",
    "        else:\n",
    "            train_dga_i.extend(df_dga_fam.iloc[0:int(ratio*len(df_dga_fam))].index)\n",
    "            test_dga_i.extend(df_dga_fam.iloc[int(ratio*len(df_dga_fam)):].index)\n",
    "    df_legit = df_legit.sample(frac = 1)\n",
    "    train_ben_i.extend(df_legit.iloc[0:int(ratio*len(df_legit))].index)\n",
    "    test_ben_i.extend(df_legit.iloc[int(ratio*len(df_legit)):].index)\n",
    "    train_dga_i.extend(train_ben_i)\n",
    "    test_dga_i.extend(test_ben_i)\n",
    "    X_train = df['domain_1'][train_dga_i]\n",
    "    y_train = df['label'][train_dga_i]\n",
    "    X_test = df['domain_1'][test_dga_i]\n",
    "    y_test = df['label'][test_dga_i]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "# Create data loader for the neural-net\n",
    "def create_data_loader(data,label):\n",
    "    tensor_data = torch.Tensor(data.astype(int))\n",
    "    tensor_label = torch.Tensor(label)\n",
    "    my_dataset = TensorDataset(tensor_data,tensor_label)\n",
    "    data_loader = DataLoader(my_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    return data_loader\n",
    "# Create batches for Siamese networks\n",
    "def create_batch_offline(indices,batch_size,anc_indices,domain_indices=None):\n",
    "    \"\"\"choose an anchor, a positive and a negative batch.\n",
    "    if domain_indices is given, choose anchor only from the specified domains indices. \"\"\"\n",
    "    x_anchors = np.zeros((batch_size, 75))\n",
    "    x_positives = np.zeros_like(x_anchors)\n",
    "    x_negatives = np.zeros_like(x_anchors)\n",
    "\n",
    "    y = encoded_labels[indices]\n",
    "    anc_indices = np.intersect1d(anc_indices,domain_indices,assume_unique=True)\n",
    "    for i in range(0, batch_size):\n",
    "        anc_idx = np.random.choice(anc_indices) \n",
    "        x_anchor = X_data[anc_idx]\n",
    "        y_anchor = encoded_labels[anc_idx]\n",
    "\n",
    "        indices_for_pos = indices[np.where(y == y_anchor)]  #resulting array alway >=1 (the anchor itself)\n",
    "        pos_idx = np.random.choice(indices_for_pos)\n",
    "        indices_for_neg = indices[np.where(y != y_anchor)] \n",
    "        neg_idx = np.random.choice(indices_for_neg)\n",
    "\n",
    "        x_positive = X_data[pos_idx]\n",
    "        x_negative = X_data[neg_idx] \n",
    "        \n",
    "        x_anchors[i] = x_anchor\n",
    "        x_positives[i] = x_positive\n",
    "        x_negatives[i] = x_negative\n",
    "        \n",
    "    return [x_anchors, x_positives, x_negatives]\n",
    "\n",
    "# Generation of Triplets for Triplet-loss\n",
    "def triplets_generator(**kwargs):\n",
    "    while True:\n",
    "        x = create_batch_offline(**kwargs)\n",
    "        dummy_y = np.zeros((x[0].shape[0], 3 , EMB_SIZE))  #dummy y (never used) the size of the siamese input is required\n",
    "        yield x,dummy_y\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor, positive, negative = y_pred[:,:EMB_SIZE], y_pred[:,EMB_SIZE:2*EMB_SIZE], y_pred[:,2*EMB_SIZE:]\n",
    "    p_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "    n_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "    return tf.reduce_mean(tf.maximum(0., p_dist - n_dist + alpha))        \n",
    "# Calculate the distance from centroid        \n",
    "def argmin_label(row, ref, ref_save):\n",
    "    emb = np.array(row[[0,1,2,3,4,5,6,7,8,9]])\n",
    "    list_dist = [np.sum(np.power(emb-ref_save[key],2)) for key in ref]\n",
    "    arg_m = np.argmin(list_dist)\n",
    "    dist_m = np.min(list_dist)\n",
    "    row['predict_label'] = list(domains)[arg_m]\n",
    "    row['predict_dist'] = dist_m\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Processing the binary data\n",
    "def data_preprocessing_binary(df):\n",
    "    # Make spaces between domain chars\n",
    "    df['domain_1'] = df['domain'].apply(get_domain_space)\n",
    "    # Split train-test\n",
    "    X_train, X_test, y_train, y_test = split_train_test_dga(df,0.8)\n",
    "\n",
    "    domain_test = df['domain'].iloc[X_test.index]\n",
    "    type_test = df['type'].iloc[X_test.index]\n",
    "    # Convert text to tokens\n",
    "    X_train_np = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train_np = pad_sequences(X_train_np, maxlen=75, padding='post')\n",
    "    X_test_np = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test_np = pad_sequences(X_test_np, maxlen=75, padding='post')\n",
    "    \n",
    "    X_train = np.array(X_train_np).astype(int)\n",
    "    X_test = np.array(X_test_np).astype(int)\n",
    "    \n",
    "    train_loader = create_data_loader(X_train, list(y_train))\n",
    "    test_loader = create_data_loader(X_test, list(y_test))\n",
    "            \n",
    "    return X_train, y_train, X_test, y_test, domain_test, type_test\n",
    "\n",
    "# Train the binary model\n",
    "def train_model_binary(X_train, y_train, X_test, y_test):\n",
    "    # Defining the model\n",
    "    inputA = tf.keras.layers.Input(shape=(X_train.shape[1],), name='input')\n",
    "    x = tf.keras.layers.Embedding(max_features, 128, input_length=75)(inputA)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "    x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "    model = tf.keras.Model(inputs=x.input, outputs=x.output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #batch_size = 1000\n",
    "    model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "             validation_data=([X_test], y_test), class_weight=CLASS_WEIGHTS)\n",
    "    return model\n",
    "# Evaluate the binary model\n",
    "def model_eval_binary(model, X_test, y_test, domain_test, type_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    D_test = pd.DataFrame()\n",
    "    \n",
    "    D_test[\"domain\"] = domain_test\n",
    "    D_test[\"type\"] = type_test\n",
    "    D_test[\"label\"] = y_test\n",
    "    D_test[\"pred\"] = y_pred\n",
    "    \n",
    "    recall = []\n",
    "    precision = []\n",
    "    ratio_malicious_benign = 0.01\n",
    "    flag_pass = False\n",
    "    thr_final = 0\n",
    "    for thr in np.arange(0, 1, 0.01):\n",
    "        FPs = len(D_test[(D_test['pred']>thr) & (D_test['label']==0)])\n",
    "        len_ben = len(D_test[D_test['label']==0])\n",
    "        len_mal = len(D_test[D_test['label']==0])*ratio_malicious_benign\n",
    "        recall_step = len(D_test[(D_test['pred']>thr) & (D_test['label']==1)])/len(D_test[D_test['label']==1])\n",
    "        recall.append(recall_step)\n",
    "        TPs = len_mal*recall_step\n",
    "        precision_score = TPs / (TPs + FPs + 0.0000001) # handle div by zero\n",
    "        precision.append(precision_score)\n",
    "        if TPs/(TPs+FPs) > 0.9 and flag_pass == False:\n",
    "            print('Precision: {}'.format(precision_score))\n",
    "            print('Recall: {}'.format(recall_step))\n",
    "            print('Threshhold: {}'.format(thr))\n",
    "            thr_final = thr\n",
    "            flag_pass = True\n",
    "    pyplot.plot(recall, precision, marker='.', label='CNN Pytorch')\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    pyplot.title('DGA model')\n",
    "       \n",
    "    D_test_mal = pd.DataFrame(D_test[D_test['label']==1].groupby(['type'], as_index=False)['label'].sum())\n",
    "    D_test_mal_detected = pd.DataFrame(D_test[(D_test['label']==1) & (D_test['pred']>thr_final)].groupby(['type'], as_index=False)['label'].sum())\n",
    "    D_test_mal_detected.columns = ['type','detected']\n",
    "    D_test_mal = pd.merge(D_test_mal, D_test_mal_detected,how = \"left\", on=[\"type\"])\n",
    "    D_test_mal['detected'] = D_test_mal['detected'].fillna(0)\n",
    "    D_test_mal['ratio'] = D_test_mal['detected']/D_test_mal['label']\n",
    "    print(D_test_mal[(D_test_mal['ratio']<thr_final) & (D_test_mal['label']>D_test_mal['label'].median())])\n",
    "    print(D_test_mal[(D_test_mal['ratio']>thr_final) & (D_test_mal['label']>D_test_mal['label'].median())])\n",
    "\n",
    "# Processing the families data\n",
    "def data_preprocessing_families(df):    \n",
    "    # Merge dgas families with the same pattern\n",
    "    df['type'] = df['type'].replace('FluBot_dga','flubot')\n",
    "    df['type'] = df['type'].replace('fobber_v2','fobber')\n",
    "    df['type'] = df['type'].replace('legit','alexa')\n",
    "    df['type'] = df['type'].replace('pykspa_v2_real','pykspa')\n",
    "    df['type'] = df['type'].replace('pykspa_v2_fake','pykspa')\n",
    "    df['type'] = df['type'].replace('gameoverdga','gameover')\n",
    "    # Merge others dgas families to 1 label\n",
    "    for type_dga in pd.unique(df['type']):\n",
    "        if type_dga not in['goz','bazarbackdoor','bamital','gspy','dyre','enviserv','chinad','monerodownloader','emotet','ramdo','padcrypt','qadars','banjori','corebot','rovnix','flubot','gameover','alexa']:\n",
    "            df['type'] = df['type'].replace(type_dga,'alexa')\n",
    "    \n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    labels_type = df['type']\n",
    "    # Label encoding the dga families\n",
    "    le = LabelEncoder()\n",
    "    encoded_labels = le.fit_transform(labels_type)\n",
    "    dict_type_count = df['type'].value_counts().to_dict()\n",
    "    dict_type_count.pop('alexa')\n",
    "    df['domain_1'] = df['domain'].apply(get_domain_space)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_train_test_dga(df,0.8)\n",
    "       \n",
    "    # Convert text to tokens\n",
    "    X_data = tokenizer.texts_to_sequences(df['domain_1'])\n",
    "    X_data = pad_sequences(X_data, maxlen=75, padding='post')\n",
    "    domains = dict_type_count.keys()\n",
    "    df['new_col'] = df['type'].isin(domains).astype(int)\n",
    "    domains_idx = np.array(df.index[df['new_col'] == 1])\n",
    "    noise_idx = np.array(df.index[df['new_col'] == 0])\n",
    "    indices = domains_idx\n",
    "    train_indices_same = np.intersect1d(X_train.index, domains_idx, assume_unique=False)\n",
    "    train_indices_diff = np.intersect1d(X_train.index, noise_idx, assume_unique=False)\n",
    "    test_indices_same = np.intersect1d(X_test.index, domains_idx, assume_unique=False)\n",
    "    test_indices_diff = np.intersect1d(X_test.index, noise_idx, assume_unique=False) \n",
    "    train_classes, train_cnt = np.unique(encoded_labels[train_indices_same], return_counts=True)\n",
    "    test_classes, test_cnt = np.unique(encoded_labels[test_indices_same], return_counts=True)\n",
    "    stacked = np.stack((train_cnt,test_cnt),axis=1)\n",
    "    \n",
    "    anc_idx = np.random.choice(train_indices_same) \n",
    "    anchor = X_data[anc_idx]\n",
    "    encoded_labels_train = encoded_labels[domains_idx]\n",
    "    steps_per_epoch = int(train_indices_same.size/TRAIN_BATCH_SIZE)\n",
    "    validation_steps = int(test_indices_same.size/TEST_BATCH_SIZE)\n",
    "\n",
    "    train_generator = triplets_generator(indices=X_train.index,batch_size=TRAIN_BATCH_SIZE,anc_indices=train_indices_same,domain_indices=train_indices_same)\n",
    "    validation_generator = triplets_generator(indices=X_test.index,batch_size=TEST_BATCH_SIZE,anc_indices=test_indices_same,domain_indices=test_indices_same)\n",
    "    \n",
    "    return train_generator, validation_generator, X_data, encoded_labels, steps_per_epoch, domains, labels_type, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff\n",
    "\n",
    "# Train the families model\n",
    "def train_model_families(train_generator, steps_per_epoch):  \n",
    "    # Defining the model\n",
    "    inputA = tf.keras.layers.Input(shape=(75,), name='input')\n",
    "    x = tf.keras.layers.Embedding(max_features, 128, input_length=75)(inputA)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(EMB_SIZE, activation=None)(x)\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x,axis=1), name='output')(x)\n",
    "    x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "    model = tf.keras.Model(inputs=x.input, outputs=x.output)\n",
    "    \n",
    "    input_anchor = tf.keras.layers.Input(shape=(75))\n",
    "    input_positive = tf.keras.layers.Input(shape=(75))\n",
    "    input_negative = tf.keras.layers.Input(shape=(75))\n",
    "\n",
    "    embedding_anchor = model(input_anchor)\n",
    "    embedding_positive = model(input_positive)\n",
    "    embedding_negative = model(input_negative)\n",
    "\n",
    "    output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
    "\n",
    "    siamese_net = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)\n",
    "    \n",
    "    siamese_net.compile(loss=triplet_loss, optimizer=Adam(learning_rate=LEARNING_RATE_SIAMESE))\n",
    "    \n",
    "    history = siamese_net.fit(\n",
    "    train_generator, steps_per_epoch=steps_per_epoch, epochs=EPOCHS_SIAMESE, workers=8 ,use_multiprocessing=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluate families model\n",
    "def model_eval_families(model, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff):\n",
    "    train_indices_same = np.sort(train_indices_same)\n",
    "    train_indices_diff = np.sort(train_indices_diff)\n",
    "    x_train_same = X_data[train_indices_same]\n",
    "    x_train_diff = X_data[train_indices_diff]\n",
    "    y_train_same = labels_type.iloc[train_indices_same]\n",
    "    y_train_diff = labels_type.iloc[train_indices_diff]\n",
    "    x_test_same = X_data[test_indices_same]\n",
    "    x_test_diff = X_data[test_indices_diff]\n",
    "    y_test_same = labels_type.iloc[test_indices_same]\n",
    "    y_test_diff = labels_type.iloc[test_indices_diff]\n",
    "        \n",
    "    x_train_same_emb = model.predict(x_train_same)\n",
    "    x_train_diff_emb = model.predict(x_train_diff)\n",
    "    \n",
    "    x_test_same_emb = model.predict(x_test_same)\n",
    "    x_test_diff_emb = model.predict(x_test_diff)\n",
    "    \n",
    "    # Create a dict of the output model\n",
    "    #create a dict of vector embeddings per class:\n",
    "    ref ={}\n",
    "    for domain in domains:\n",
    "        x_domain = x_train_same[np.where(y_train_same == domain)[0]]\n",
    "        ref[domain] = model(x_domain)\n",
    "    ref_save = {}\n",
    "    # Create dict of anchors\n",
    "    for key in ref:\n",
    "        ref_save[key] = ref[key][0]\n",
    "    ref_save_df = pd.DataFrame()\n",
    "    ref_save_df['Family'] = ref_save.keys()\n",
    "    for i in range(len(ref_save['emotet'])):\n",
    "        list_vec = []\n",
    "        for key in ref_save:\n",
    "            list_vec.append(ref_save[key][i].numpy())\n",
    "        ref_save_df[i] = list_vec\n",
    "    \n",
    "    y_test_same_list = y_test_same.tolist()\n",
    "    df_test_same_emb = pd.DataFrame(x_test_same_emb)\n",
    "    df_test_same_emb['label'] = y_test_same_list\n",
    "    df_test_same_emb_mini = df_test_same_emb\n",
    "    df_test_same_emb_mini = df_test_same_emb_mini.swifter.apply(lambda row : argmin_label(row, ref, ref_save),axis=1)\n",
    "    \n",
    "    print(len(df_test_same_emb_mini[(df_test_same_emb_mini['predict_label']==df_test_same_emb_mini['label']) & (df_test_same_emb_mini['predict_dist']<0.5)])/len(df_test_same_emb_mini))\n",
    "    \n",
    "    y_test_diff_list = y_test_diff.tolist()\n",
    "    df_test_diff_emb = pd.DataFrame(x_test_diff_emb)\n",
    "    df_test_diff_emb['label'] = y_test_diff_list\n",
    "    df_test_diff_emb_mini = df_test_diff_emb\n",
    "    df_test_diff_emb_mini = df_test_diff_emb_mini.swifter.apply(lambda row : argmin_label(row, ref, ref_save),axis=1)\n",
    "    \n",
    "    print(len(df_test_diff_emb_mini[(df_test_diff_emb_mini['predict_dist']<0.5) & ~df_test_diff_emb_mini['predict_label'].isin(['simda','fobber','pykspa_v1'])])/len(df_test_same_emb_mini[(~df_test_same_emb_mini['label'].isin(['simda','fobber','pykspa_v1'])) & (df_test_same_emb_mini['predict_label']==df_test_same_emb_mini['label']) & (df_test_same_emb_mini['predict_dist']<0.5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaOrtbm-F42x"
   },
   "source": [
    "### Prepare training dataset\n",
    " - Read tokenizer\n",
    " - Read dga training dataset as CSV format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NF4MTVA0CBD2",
    "outputId": "c57736bb-420c-4456-d7a7-d31dd587b233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tokenizer...\n",
      "Reading data for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Read tokenizer\n",
    "print(\"Reading tokenizer...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.word_index = pd.read_csv('../models/tokenizer.csv').set_index('keys')['values'].to_dict()\n",
    "max_features = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Reading data for training\")\n",
    "df_binary = pd.read_csv(\"../datasets/dga_training_dataset.csv\")\n",
    "df_families = df_binary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9DuHUfhGOX6"
   },
   "source": [
    "## Binary model based DGA\n",
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5IYjShJLCBD7",
    "outputId": "2c98434e-725a-4f91-fb2c-5566cf8b93b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for binary model...\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing data for binary model...\")\n",
    "X_train, y_train, X_test, y_test, domain_test, type_test = data_preprocessing_binary(df_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3s7-d2DG1JA"
   },
   "source": [
    "### Training and evaluation of binary DGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "M0_v2MkkCBD9",
    "outputId": "94fb6425-fa87-440b-f1b6-6d9540fca79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training binary model...\n",
      "Epoch 1/30\n",
      "997/997 [==============================] - 125s 125ms/step - loss: 1.8102 - accuracy: 0.7177 - val_loss: 2.0533 - val_accuracy: 0.1049\n",
      "Epoch 2/30\n",
      "997/997 [==============================] - 126s 126ms/step - loss: 0.9358 - accuracy: 0.7349 - val_loss: 1.4305 - val_accuracy: 0.5864\n",
      "Epoch 3/30\n",
      "997/997 [==============================] - 125s 125ms/step - loss: 0.8458 - accuracy: 0.8459 - val_loss: 1.3955 - val_accuracy: 0.5552\n",
      "Epoch 4/30\n",
      "997/997 [==============================] - 126s 126ms/step - loss: 0.7956 - accuracy: 0.8577 - val_loss: 1.4289 - val_accuracy: 0.5793\n",
      "Epoch 5/30\n",
      "997/997 [==============================] - 127s 127ms/step - loss: 0.7581 - accuracy: 0.8668 - val_loss: 0.9327 - val_accuracy: 0.7611\n",
      "Epoch 6/30\n",
      "997/997 [==============================] - 126s 126ms/step - loss: 0.7242 - accuracy: 0.8742 - val_loss: 0.8944 - val_accuracy: 0.7659\n",
      "Epoch 7/30\n",
      "997/997 [==============================] - 125s 125ms/step - loss: 0.6948 - accuracy: 0.8793 - val_loss: 0.7241 - val_accuracy: 0.8230\n",
      "Epoch 8/30\n",
      "997/997 [==============================] - 127s 127ms/step - loss: 0.6642 - accuracy: 0.8844 - val_loss: 0.6824 - val_accuracy: 0.8216\n",
      "Epoch 9/30\n",
      "997/997 [==============================] - 126s 127ms/step - loss: 0.6390 - accuracy: 0.8882 - val_loss: 0.5836 - val_accuracy: 0.8452\n",
      "Epoch 10/30\n",
      "997/997 [==============================] - 125s 126ms/step - loss: 0.6204 - accuracy: 0.8913 - val_loss: 0.5465 - val_accuracy: 0.8506\n",
      "Epoch 11/30\n",
      "997/997 [==============================] - 127s 128ms/step - loss: 0.5944 - accuracy: 0.8943 - val_loss: 0.5350 - val_accuracy: 0.8351\n",
      "Epoch 12/30\n",
      "997/997 [==============================] - 127s 127ms/step - loss: 0.5790 - accuracy: 0.8968 - val_loss: 0.4136 - val_accuracy: 0.8821\n",
      "Epoch 13/30\n",
      "997/997 [==============================] - 127s 128ms/step - loss: 0.5600 - accuracy: 0.8998 - val_loss: 0.3747 - val_accuracy: 0.8961\n",
      "Epoch 14/30\n",
      "997/997 [==============================] - 127s 127ms/step - loss: 0.5480 - accuracy: 0.9020 - val_loss: 0.5916 - val_accuracy: 0.8214\n",
      "Epoch 15/30\n",
      "997/997 [==============================] - 126s 126ms/step - loss: 0.5360 - accuracy: 0.9035 - val_loss: 0.3369 - val_accuracy: 0.9079\n",
      "Epoch 16/30\n",
      "997/997 [==============================] - 127s 127ms/step - loss: 0.5266 - accuracy: 0.9051 - val_loss: 0.3837 - val_accuracy: 0.8935\n",
      "Epoch 17/30\n",
      "997/997 [==============================] - 126s 126ms/step - loss: 0.5162 - accuracy: 0.9067 - val_loss: 0.3007 - val_accuracy: 0.9172\n",
      "Epoch 18/30\n",
      "997/997 [==============================] - 126s 126ms/step - loss: 0.5093 - accuracy: 0.9080 - val_loss: 0.3608 - val_accuracy: 0.9007\n",
      "Epoch 19/30\n",
      "997/997 [==============================] - 202s 203ms/step - loss: 0.4968 - accuracy: 0.9100 - val_loss: 0.3520 - val_accuracy: 0.9030\n",
      "Epoch 20/30\n",
      "997/997 [==============================] - 214s 214ms/step - loss: 0.4929 - accuracy: 0.9110 - val_loss: 0.3277 - val_accuracy: 0.9120\n",
      "Epoch 21/30\n",
      "997/997 [==============================] - 215s 216ms/step - loss: 0.4892 - accuracy: 0.9117 - val_loss: 0.3895 - val_accuracy: 0.8943\n",
      "Epoch 22/30\n",
      "997/997 [==============================] - 215s 216ms/step - loss: 0.4805 - accuracy: 0.9130 - val_loss: 0.3666 - val_accuracy: 0.9018\n",
      "Epoch 23/30\n",
      "997/997 [==============================] - 213s 214ms/step - loss: 0.4786 - accuracy: 0.9136 - val_loss: 0.3367 - val_accuracy: 0.9103\n",
      "Epoch 24/30\n",
      "997/997 [==============================] - 238s 239ms/step - loss: 0.4695 - accuracy: 0.9151 - val_loss: 0.3532 - val_accuracy: 0.9020\n",
      "Epoch 25/30\n",
      "997/997 [==============================] - 236s 237ms/step - loss: 0.4640 - accuracy: 0.9158 - val_loss: 0.3295 - val_accuracy: 0.9114\n",
      "Epoch 26/30\n",
      "997/997 [==============================] - 194s 194ms/step - loss: 0.4639 - accuracy: 0.9157 - val_loss: 0.3312 - val_accuracy: 0.9171\n",
      "Epoch 27/30\n",
      "997/997 [==============================] - 231s 232ms/step - loss: 0.4587 - accuracy: 0.9170 - val_loss: 0.3193 - val_accuracy: 0.9111\n",
      "Epoch 28/30\n",
      "997/997 [==============================] - 236s 237ms/step - loss: 0.4537 - accuracy: 0.9176 - val_loss: 0.3031 - val_accuracy: 0.9226\n",
      "Epoch 29/30\n",
      "997/997 [==============================] - 239s 240ms/step - loss: 0.4545 - accuracy: 0.9175 - val_loss: 0.2982 - val_accuracy: 0.9218\n",
      "Epoch 30/30\n",
      "997/997 [==============================] - 239s 240ms/step - loss: 0.4499 - accuracy: 0.9185 - val_loss: 0.3724 - val_accuracy: 0.9060\n"
     ]
    }
   ],
   "source": [
    "print(\"Training binary model...\")\n",
    "model_binary = train_model_binary(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating binary model...\n",
      "53301/53301 [==============================] - 174s 3ms/step\n",
      "Precision: 0.9016803129828288\n",
      "Recall: 0.9020375492815489\n",
      "Threshhold: 0.42\n",
      "         type  label  detected     ratio\n",
      "9   conficker   5996    1365.0  0.227652\n",
      "22       gozi   5500    1786.0  0.324727\n",
      "36     nymaim   5821     105.0  0.018038\n",
      "40     pushdo   5500    1752.0  0.318545\n",
      "49      simda  21132    8143.0  0.385340\n",
      "50   suppobox   7415     614.0  0.082805\n",
      "51      symmi   9756    2947.0  0.302071\n",
      "58      virut   1950     183.0  0.093846\n",
      "            type   label  detected     ratio\n",
      "0     FluBot_dga   82000   76818.0  0.936805\n",
      "3        banjori  461986  429488.0  0.929656\n",
      "10       corebot    5500    5340.0  0.970909\n",
      "11  cryptolocker   43569   38283.0  0.878675\n",
      "12      dircrypt    6054    5050.0  0.834159\n",
      "14        emotet  463549  444013.0  0.957856\n",
      "17        flubot   21997   20574.0  0.935309\n",
      "18        fobber    5784    4296.0  0.742739\n",
      "19      gameover    4000    3987.0  0.996750\n",
      "27        matsnu    5941    2583.0  0.434775\n",
      "29       murofet   10281    9994.0  0.972084\n",
      "32        necurs   21717   17167.0  0.790487\n",
      "33        newgoz    1856    1848.0  0.995690\n",
      "38      padcrypt    5668    5231.0  0.922900\n",
      "41        pykspa   37983   31398.0  0.826633\n",
      "42        qadars    7300    6853.0  0.938767\n",
      "43         ramdo    5500    5267.0  0.957636\n",
      "44        ramnit   15632   12741.0  0.815059\n",
      "45       ranbyus   16300   15394.0  0.944417\n",
      "46        rovnix  171995  167388.0  0.973214\n",
      "53         tinba   52322   45852.0  0.876343\n",
      "56       vawtrak    6220    3582.0  0.575884\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Evaluating binary model...\")\n",
    "model_eval_binary(model_binary, X_test, y_test, domain_test, type_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_tjDG4WHARC",
    "tags": []
   },
   "source": [
    "## Detection based on families model DGA\n",
    "\n",
    "### Data processing and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IZOzqHqYCBEA",
    "outputId": "1ec320aa-36cc-4a5b-89b5-60e781aa5bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for families model...\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing data for families model...\")\n",
    "train_generator, validation_generator, X_data, encoded_labels, steps_per_epoch, domains, labels_type, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff = data_preprocessing_families(df_families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training families model...\n"
     ]
    }
   ],
   "source": [
    "# Takes long to run\n",
    "print(\"Training families model...\")\n",
    "model_families = train_model_families(train_generator, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clbkRRkYHtua",
    "tags": []
   },
   "source": [
    "### Evaluation of the families model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "2db5860e920e41d1b3780f8be7e63636",
      "765dcced9b6d48b8b7052f338c8cd046"
     ]
    },
    "id": "iKAX7GVKCBEB",
    "outputId": "05f29b1c-37c9-4493-c505-ca5361d59b06"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating families model...\")\n",
    "model_eval_families(model_families, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1S_3hCu6QPl"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this work we show an example of detecting character based DGA algorithms with high Precision 0.9, Recall 0.9 and accuracy 0.95 of detecting many character DGA families. In future work, we plan to develop a word-based DGA model, that could tackle the limitation of character based DGA models. Our model is tested on AppShield - BlueField, an agentless system. By using AppShield we succeeded to detect DGA malwares before the malware connects with the command and control server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bf5AZ5Pl7iEm"
   },
   "source": [
    "# References\n",
    "- https://data.netlab.360.com/dga/\n",
    "- https://underdefense.com/guides/detecting-dga-domains-machine-learning-approach/\n",
    "- https://developer.nvidia.com/networking/doca \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dga-appshield-cnn-20220214.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

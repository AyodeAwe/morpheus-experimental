{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DGA detection model"
      ],
      "metadata": {
        "id": "c1wxCnfvDjfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "* Introduction\n",
        "* Dataset\n",
        "* Data Preprocessing\n",
        "* Binary Model Training and Evaluation\n",
        "* Familieis Model Training and Evaluation\n",
        "* Conclusions\n",
        "* References"
      ],
      "metadata": {
        "id": "-ViGLx7pDw4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Domain generation algorithms (DGA) are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. The large number of potential rendezvous points makes it difficult for law enforcement to effectively shut down botnets, since infected computers will attempt to contact some of these domain names every day to receive updates or commands. The use of public-key cryptography in malware code makes it unfeasible for law enforcement and other actors to mimic commands from the malware controllers as some worms will automatically reject any updates not signed by the malware controllers."
      ],
      "metadata": {
        "id": "1AnMgLK4D6AL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "We gathered 3M domains from popular datasets of DGAs.\n",
        "i.e. https://data.netlab.360.com/dga/"
      ],
      "metadata": {
        "id": "2wJonJ9RER7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports installation"
      ],
      "metadata": {
        "id": "nvA-XuWf79zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required dependencies\n",
        "! pip install tldextract\n",
        "! pip install tensorflow\n",
        "! pip install dgaintel\n",
        "! pip install torch\n",
        "! pip install swifter"
      ],
      "metadata": {
        "id": "qtWhJU8N78eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "VoqOysmTFKo6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FCq782QCBDf"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tldextract\n",
        "\n",
        "import glob\n",
        "\n",
        "from dgaintel import get_prob\n",
        "\n",
        "import swifter\n",
        "\n",
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import tldextract\n",
        "\n",
        "import swifter\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import glob\n",
        "\n",
        "from dgaintel import get_prob\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Flatten\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers \n",
        "from tensorflow.keras.optimizers import SGD,RMSprop,Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "t-QbjcHdFhiG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKI3cdJ7CBDi"
      },
      "outputs": [],
      "source": [
        "# Binary model params\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 1000\n",
        "MALICIOUS_RATIO = 0.01\n",
        "LEARNING_RATE = 0.001\n",
        "# Families model params\n",
        "TRAIN_BATCH_SIZE = 500\n",
        "TEST_BATCH_SIZE = 500\n",
        "EMB_SIZE = 10\n",
        "EPOCHS_SIAMESE = 120\n",
        "LEARNING_RATE_SIAMESE = 1e-3\n",
        "CLASS_WEIGHTS = {0: 100, 1:1}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "afOzj7DVFyyb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GcBS0clCBDk"
      },
      "outputs": [],
      "source": [
        "# Return URL domain\n",
        "def get_domain(url):\n",
        "    domain = tldextract.extract(url).domain\n",
        "    if domain == 'ddns':\n",
        "        print(url)\n",
        "        urls = url.split('.')\n",
        "        urls_i = urls.index('ddns')\n",
        "        if urls_i == 0:\n",
        "            return 'ddns'\n",
        "        print(urls[urls_i-1])\n",
        "        return urls[urls_i-1]\n",
        "    if domain:\n",
        "        return domain\n",
        "    return ''\n",
        "# Make spaces for character processing\n",
        "def get_domain_space(domain):\n",
        "    try:\n",
        "        return \" \".join(domain)\n",
        "    except:\n",
        "        print(domain)\n",
        "        return \"\"\n",
        "# Split train-test - we are trying change the ratio for small DGA families to test them too \n",
        "def split_train_test_dga(df,ratio=0.8):\n",
        "    df_dga = df[df['label']==1]\n",
        "    df_legit = df[df['label']==0]\n",
        "    X_dga, y_dga = df_dga['domain_1'],df_dga['label']\n",
        "    X_legit, y_legit = df_legit['domain_1'],df_legit['label']\n",
        "    train_dga_i = []\n",
        "    train_ben_i = []\n",
        "    test_dga_i = []\n",
        "    test_ben_i = []\n",
        "    # Make the dga train set to be more equale between families without dominant family\n",
        "    for fam in pd.unique(df_dga['type']):\n",
        "        df_dga_fam = df_dga[df_dga['type']==fam]\n",
        "        # Shuffle the dataframe rows\n",
        "        df_dga_fam = df_dga_fam.sample(frac = 1)\n",
        "        if len(df_dga_fam)>10000:\n",
        "            train_dga_i.extend(df_dga_fam.iloc[0:int(ratio*10000)].index)\n",
        "            test_dga_i.extend(df_dga_fam.iloc[int(ratio*10000):].index)\n",
        "        else:\n",
        "            train_dga_i.extend(df_dga_fam.iloc[0:int(ratio*len(df_dga_fam))].index)\n",
        "            test_dga_i.extend(df_dga_fam.iloc[int(ratio*len(df_dga_fam)):].index)\n",
        "    df_legit = df_legit.sample(frac = 1)\n",
        "    train_ben_i.extend(df_legit.iloc[0:int(ratio*len(df_legit))].index)\n",
        "    test_ben_i.extend(df_legit.iloc[int(ratio*len(df_legit)):].index)\n",
        "    train_dga_i.extend(train_ben_i)\n",
        "    test_dga_i.extend(test_ben_i)\n",
        "    X_train = df['domain_1'][train_dga_i]\n",
        "    y_train = df['label'][train_dga_i]\n",
        "    X_test = df['domain_1'][test_dga_i]\n",
        "    y_test = df['label'][test_dga_i]\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "# Create data loader for the neural-net\n",
        "def create_data_loader(data,label):\n",
        "    tensor_data = torch.Tensor(data.astype(int))\n",
        "    tensor_label = torch.Tensor(label)\n",
        "    my_dataset = TensorDataset(tensor_data,tensor_label)\n",
        "    data_loader = DataLoader(my_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return data_loader\n",
        "# Create batches for Siamese networks\n",
        "def create_batch_offline(indices,batch_size,anc_indices,domain_indices=None):\n",
        "    \"\"\"choose an anchor, a positive and a negative batch.\n",
        "    if domain_indices is given, choose anchor only from the specified domains indices. \"\"\"\n",
        "    x_anchors = np.zeros((batch_size, 75))\n",
        "    x_positives = np.zeros_like(x_anchors)\n",
        "    x_negatives = np.zeros_like(x_anchors)\n",
        "\n",
        "    y = encoded_labels[indices]\n",
        "    anc_indices = np.intersect1d(anc_indices,domain_indices,assume_unique=True)\n",
        "    for i in range(0, batch_size):\n",
        "        anc_idx = np.random.choice(anc_indices) \n",
        "        x_anchor = X_data[anc_idx]\n",
        "        y_anchor = encoded_labels[anc_idx]\n",
        "\n",
        "        indices_for_pos = indices[np.where(y == y_anchor)]  #resulting array alway >=1 (the anchor itself)\n",
        "        pos_idx = np.random.choice(indices_for_pos)\n",
        "        indices_for_neg = indices[np.where(y != y_anchor)] \n",
        "        neg_idx = np.random.choice(indices_for_neg)\n",
        "\n",
        "        x_positive = X_data[pos_idx]\n",
        "        x_negative = X_data[neg_idx] \n",
        "        \n",
        "        x_anchors[i] = x_anchor\n",
        "        x_positives[i] = x_positive\n",
        "        x_negatives[i] = x_negative\n",
        "        \n",
        "    return [x_anchors, x_positives, x_negatives] \n",
        "# Generation of Triplets for Triplet-loss\n",
        "def triplets_generator(**kwargs):\n",
        "    while True:\n",
        "        x = create_batch_offline(**kwargs)\n",
        "        dummy_y = np.zeros((x[0].shape[0], 3 , EMB_SIZE))  #dummy y (never used) the size of the siamese input is required\n",
        "        yield x,dummy_y\n",
        "# Triplet loss\n",
        "alpha = 1.5  #value between 0-2\n",
        "def triplet_loss(y_true, y_pred):\n",
        "    anchor, positive, negative = y_pred[:,:EMB_SIZE], y_pred[:,EMB_SIZE:2*EMB_SIZE], y_pred[:,2*EMB_SIZE:]\n",
        "    p_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
        "    n_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
        "    return tf.reduce_mean(tf.maximum(0., p_dist - n_dist + alpha))        \n",
        "# Calculate the distance from centroid        \n",
        "def argmin_label(row, ref, ref_save):\n",
        "    emb = np.array(row[[0,1,2,3,4,5,6,7,8,9]])\n",
        "    list_dist = [np.sum(np.power(emb-ref_save[key],2)) for key in ref]\n",
        "    arg_m = np.argmin(list_dist)\n",
        "    dist_m = np.min(list_dist)\n",
        "    row['predict_label'] = list(domains)[arg_m]\n",
        "    row['predict_dist'] = dist_m\n",
        "    return row\n",
        "# Processing the binary data\n",
        "def data_preprocessing_binary(df):\n",
        "    # Make spaces between domain chars\n",
        "    df['domain_1'] = df['domain'].apply(get_domain_space)\n",
        "    # Split train-test\n",
        "    X_train, X_test, y_train, y_test = split_train_test_dga(df,0.8)\n",
        "\n",
        "    domain_test = df['domain'].iloc[X_test.index]\n",
        "    type_test = df['type'].iloc[X_test.index]\n",
        "    # Convert text to tokens\n",
        "    X_train_np = tokenizer.texts_to_sequences(X_train)\n",
        "    X_train_np = pad_sequences(X_train_np, maxlen=75, padding='post')\n",
        "    X_test_np = tokenizer.texts_to_sequences(X_test)\n",
        "    X_test_np = pad_sequences(X_test_np, maxlen=75, padding='post')\n",
        "    \n",
        "    X_train = np.array(X_train_np).astype(int)\n",
        "    X_test = np.array(X_test_np).astype(int)\n",
        "    \n",
        "    train_loader = create_data_loader(X_train, list(y_train))\n",
        "    test_loader = create_data_loader(X_test, list(y_test))\n",
        "            \n",
        "    return X_train, y_train, X_test, y_test, domain_test, type_test\n",
        "# Train the binary model\n",
        "def train_model_binary(X_train, y_train, X_test, y_test):\n",
        "    # Defining the model\n",
        "    inputA = tf.keras.layers.Input(shape=(X_train.shape[1],), name='input')\n",
        "    x = tf.keras.layers.Embedding(max_features, 128, input_length=75)(inputA)\n",
        "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "    x = tf.keras.Model(inputs=inputA, outputs=x)\n",
        "    model = tf.keras.Model(inputs=x.input, outputs=x.output)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    batch_size = 1000\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=30,\n",
        "             validation_data=([X_test], y_test), class_weight=CLASS_WEIGHTS)\n",
        "    return model\n",
        "# Evaluate the binary model\n",
        "def model_eval_binary(model, X_test, y_test, domain_test, type_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    D_test = pd.DataFrame()\n",
        "    \n",
        "    D_test[\"domain\"] = domain_test\n",
        "    D_test[\"type\"] = type_test\n",
        "    D_test[\"label\"] = y_test\n",
        "    D_test[\"pred\"] = y_pred\n",
        "    \n",
        "    recall = []\n",
        "    precision = []\n",
        "    ratio_malicious_benign = 0.01\n",
        "    flag_pass = False\n",
        "    thr_final = 0\n",
        "    for thr in np.arange(0, 1, 0.01):\n",
        "        FPs = len(D_test[(D_test['pred']>thr) & (D_test['label']==0)])\n",
        "        len_ben = len(D_test[D_test['label']==0])\n",
        "        len_mal = len(D_test[D_test['label']==0])*ratio_malicious_benign\n",
        "        recall_step = len(D_test[(D_test['pred']>thr) & (D_test['label']==1)])/len(D_test[D_test['label']==1])\n",
        "        recall.append(recall_step)\n",
        "        TPs = len_mal*recall_step\n",
        "        precision.append(TPs/(TPs+FPs))\n",
        "        if TPs/(TPs+FPs) > 0.9 and flag_pass == False:\n",
        "            print('Precision: {}'.format(TPs/(TPs+FPs)))\n",
        "            print('Recall: {}'.format(recall_step))\n",
        "            print('Threshhold: {}'.format(thr))\n",
        "            thr_final = thr\n",
        "            flag_pass = True\n",
        "    pyplot.plot(recall, precision, marker='.', label='CNN Pytorch')\n",
        "    pyplot.xlabel('Recall')\n",
        "    pyplot.ylabel('Precision')\n",
        "    pyplot.title('DGA model')\n",
        "       \n",
        "    D_test_mal = pd.DataFrame(D_test[D_test['label']==1].groupby(['type'], as_index=False)['label'].sum())\n",
        "    D_test_mal_detected = pd.DataFrame(D_test[(D_test['label']==1) & (D_test['pred']>thr_final)].groupby(['type'], as_index=False)['label'].sum())\n",
        "    D_test_mal_detected.columns = ['type','detected']\n",
        "    D_test_mal = pd.merge(D_test_mal, D_test_mal_detected,how = \"left\", on=[\"type\"])\n",
        "    D_test_mal['detected'] = D_test_mal['detected'].fillna(0)\n",
        "    D_test_mal['ratio'] = D_test_mal['detected']/D_test_mal['label']\n",
        "    print(D_test_mal[(D_test_mal['ratio']<thr_final) & (D_test_mal['label']>D_test_mal['label'].median())])\n",
        "    print(D_test_mal[(D_test_mal['ratio']>thr_final) & (D_test_mal['label']>D_test_mal['label'].median())])\n",
        "# Processing the families data\n",
        "def data_preprocessing_families(df):    \n",
        "    # Merge dgas families with the same pattern\n",
        "    df['type'] = df['type'].replace('FluBot_dga','flubot')\n",
        "    df['type'] = df['type'].replace('fobber_v2','fobber')\n",
        "    df['type'] = df['type'].replace('legit','alexa')\n",
        "    df['type'] = df['type'].replace('pykspa_v2_real','pykspa')\n",
        "    df['type'] = df['type'].replace('pykspa_v2_fake','pykspa')\n",
        "    df['type'] = df['type'].replace('gameoverdga','gameover')\n",
        "    # Merge others dgas families to 1 label\n",
        "    for type_dga in pd.unique(df['type']):\n",
        "        if type_dga not in['goz','bazarbackdoor','bamital','gspy','dyre','enviserv','chinad','monerodownloader','emotet','ramdo','padcrypt','qadars','banjori','corebot','rovnix','flubot','gameover','alexa']:\n",
        "            df['type'] = df['type'].replace(type_dga,'alexa')\n",
        "    \n",
        "    df.reset_index(drop = True, inplace = True)\n",
        "    \n",
        "    labels_type = df['type']\n",
        "    # Label encoding the dga families\n",
        "    le = LabelEncoder()\n",
        "    encoded_labels = le.fit_transform(labels_type)\n",
        "    dict_type_count = df['type'].value_counts().to_dict()\n",
        "    dict_type_count.pop('alexa')\n",
        "    df['domain_1'] = df['domain'].apply(get_domain_space)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = split_train_test_dga(df,0.8)\n",
        "       \n",
        "    # Convert text to tokens\n",
        "    X_data = tokenizer.texts_to_sequences(df['domain_1'])\n",
        "    X_data = pad_sequences(X_data, maxlen=75, padding='post')\n",
        "    domains = dict_type_count.keys()\n",
        "    df['new_col'] = df['type'].isin(domains).astype(int)\n",
        "    domains_idx = np.array(df.index[df['new_col'] == 1])\n",
        "    noise_idx = np.array(df.index[df['new_col'] == 0])\n",
        "    indices = domains_idx\n",
        "    train_indices_same = np.intersect1d(X_train.index, domains_idx, assume_unique=False)\n",
        "    train_indices_diff = np.intersect1d(X_train.index, noise_idx, assume_unique=False)\n",
        "    test_indices_same = np.intersect1d(X_test.index, domains_idx, assume_unique=False)\n",
        "    test_indices_diff = np.intersect1d(X_test.index, noise_idx, assume_unique=False) \n",
        "    train_classes, train_cnt = np.unique(encoded_labels[train_indices_same], return_counts=True)\n",
        "    test_classes, test_cnt = np.unique(encoded_labels[test_indices_same], return_counts=True)\n",
        "    stacked = np.stack((train_cnt,test_cnt),axis=1)\n",
        "    \n",
        "    anc_idx = np.random.choice(train_indices_same) \n",
        "    anchor = X_data[anc_idx]\n",
        "    encoded_labels_train = encoded_labels[domains_idx]\n",
        "    steps_per_epoch = int(train_indices_same.size/TRAIN_BATCH_SIZE)\n",
        "    validation_steps = int(test_indices_same.size/TEST_BATCH_SIZE)\n",
        "\n",
        "    train_generator = triplets_generator(indices=X_train.index,batch_size=TRAIN_BATCH_SIZE,anc_indices=train_indices_same,domain_indices=train_indices_same)\n",
        "    validation_generator = triplets_generator(indices=X_test.index,batch_size=TEST_BATCH_SIZE,anc_indices=test_indices_same,domain_indices=test_indices_same)\n",
        "    \n",
        "    return train_generator, validation_generator, X_data, encoded_labels, steps_per_epoch, domains, labels_type, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff\n",
        "# Train the families model\n",
        "def train_model_families(train_generator, steps_per_epoch):  \n",
        "    # Defining the model\n",
        "    inputA = tf.keras.layers.Input(shape=(75,), name='input')\n",
        "    x = tf.keras.layers.Embedding(max_features, 128, input_length=75)(inputA)\n",
        "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(EMB_SIZE, activation=None)(x)\n",
        "    x = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x,axis=1), name='output')(x)\n",
        "    x = tf.keras.Model(inputs=inputA, outputs=x)\n",
        "    model = tf.keras.Model(inputs=x.input, outputs=x.output)\n",
        "    \n",
        "    input_anchor = tf.keras.layers.Input(shape=(75))\n",
        "    input_positive = tf.keras.layers.Input(shape=(75))\n",
        "    input_negative = tf.keras.layers.Input(shape=(75))\n",
        "\n",
        "    embedding_anchor = model(input_anchor)\n",
        "    embedding_positive = model(input_positive)\n",
        "    embedding_negative = model(input_negative)\n",
        "\n",
        "    output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
        "\n",
        "    siamese_net = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)\n",
        "    \n",
        "    siamese_net.compile(loss=triplet_loss, optimizer=Adam(learning_rate=LEARNING_RATE_SIAMESE))\n",
        "    \n",
        "    history = siamese_net.fit(\n",
        "    train_generator, steps_per_epoch=steps_per_epoch, epochs=EPOCHS_SIAMESE, workers=8 ,use_multiprocessing=True)\n",
        "    \n",
        "    return model\n",
        "# Evaluate families model\n",
        "def model_eval_families(model, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff):\n",
        "    train_indices_same = np.sort(train_indices_same)\n",
        "    train_indices_diff = np.sort(train_indices_diff)\n",
        "    x_train_same = X_data[train_indices_same]\n",
        "    x_train_diff = X_data[train_indices_diff]\n",
        "    y_train_same = labels_type.iloc[train_indices_same]\n",
        "    y_train_diff = labels_type.iloc[train_indices_diff]\n",
        "    x_test_same = X_data[test_indices_same]\n",
        "    x_test_diff = X_data[test_indices_diff]\n",
        "    y_test_same = labels_type.iloc[test_indices_same]\n",
        "    y_test_diff = labels_type.iloc[test_indices_diff]\n",
        "        \n",
        "    x_train_same_emb = model.predict(x_train_same)\n",
        "    x_train_diff_emb = model.predict(x_train_diff)\n",
        "    \n",
        "    x_test_same_emb = model.predict(x_test_same)\n",
        "    x_test_diff_emb = model.predict(x_test_diff)\n",
        "    \n",
        "    # Create a dict of the output model\n",
        "    #create a dict of vector embeddings per class:\n",
        "    ref ={}\n",
        "    for domain in domains:\n",
        "        x_domain = x_train_same[np.where(y_train_same == domain)[0]]\n",
        "        ref[domain] = model(x_domain)\n",
        "    ref_save = {}\n",
        "    # Create dict of anchors\n",
        "    for key in ref:\n",
        "        ref_save[key] = ref[key][0]\n",
        "    ref_save_df = pd.DataFrame()\n",
        "    ref_save_df['Family'] = ref_save.keys()\n",
        "    for i in range(len(ref_save['emotet'])):\n",
        "        list_vec = []\n",
        "        for key in ref_save:\n",
        "            list_vec.append(ref_save[key][i].numpy())\n",
        "        ref_save_df[i] = list_vec\n",
        "    \n",
        "    y_test_same_list = y_test_same.tolist()\n",
        "    df_test_same_emb = pd.DataFrame(x_test_same_emb)\n",
        "    df_test_same_emb['label'] = y_test_same_list\n",
        "    df_test_same_emb_mini = df_test_same_emb\n",
        "    df_test_same_emb_mini = df_test_same_emb_mini.swifter.apply(lambda row : argmin_label(row, ref, ref_save),axis=1)\n",
        "    \n",
        "    print(len(df_test_same_emb_mini[(df_test_same_emb_mini['predict_label']==df_test_same_emb_mini['label']) & (df_test_same_emb_mini['predict_dist']<0.5)])/len(df_test_same_emb_mini))\n",
        "    \n",
        "    y_test_diff_list = y_test_diff.tolist()\n",
        "    df_test_diff_emb = pd.DataFrame(x_test_diff_emb)\n",
        "    df_test_diff_emb['label'] = y_test_diff_list\n",
        "    df_test_diff_emb_mini = df_test_diff_emb\n",
        "    df_test_diff_emb_mini = df_test_diff_emb_mini.swifter.apply(lambda row : argmin_label(row, ref, ref_save),axis=1)\n",
        "    \n",
        "    print(len(df_test_diff_emb_mini[(df_test_diff_emb_mini['predict_dist']<0.5) & ~df_test_diff_emb_mini['predict_label'].isin(['simda','fobber','pykspa_v1'])])/len(df_test_same_emb_mini[(~df_test_same_emb_mini['label'].isin(['simda','fobber','pykspa_v1'])) & (df_test_same_emb_mini['predict_label']==df_test_same_emb_mini['label']) & (df_test_same_emb_mini['predict_dist']<0.5)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "jaOrtbm-F42x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF4MTVA0CBD2",
        "outputId": "c57736bb-420c-4456-d7a7-d31dd587b233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading tokenizer...\n",
            "Reading data for training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.word_index = pd.read_csv('tokenizer.csv').set_index('keys')['values'].to_dict()\n",
        "max_features = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Reading tokenizer...\")\n",
        "# Read tokenizer\n",
        "print(\"Reading data for training\")\n",
        "df_binary = pd.read_csv(\"/raid0/haim/haim/dga_training_dataset.csv\")\n",
        "df_families = df_binary.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing"
      ],
      "metadata": {
        "id": "F9DuHUfhGOX6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IYjShJLCBD7",
        "outputId": "2c98434e-725a-4f91-fb2c-5566cf8b93b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing data for binary model...\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ]
        }
      ],
      "source": [
        "print(\"Processing data for binary model...\")\n",
        "X_train, y_train, X_test, y_test, domain_test, type_test = data_preprocessing_binary(df_binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary model training and evaluation"
      ],
      "metadata": {
        "id": "g3s7-d2DG1JA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0_v2MkkCBD9",
        "outputId": "94fb6425-fa87-440b-f1b6-6d9540fca79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training binary model...\n",
            "Epoch 1/30\n",
            "997/997 [==============================] - 88s 87ms/step - loss: 1.3752 - accuracy: 0.7181 - val_loss: 2.3266 - val_accuracy: 0.1049\n",
            "Epoch 2/30\n",
            "997/997 [==============================] - 78s 78ms/step - loss: 0.8768 - accuracy: 0.8284 - val_loss: 1.3832 - val_accuracy: 0.5919\n",
            "Epoch 3/30\n",
            "997/997 [==============================] - 73s 73ms/step - loss: 0.7938 - accuracy: 0.8583 - val_loss: 1.1017 - val_accuracy: 0.6864\n",
            "Epoch 4/30\n",
            "997/997 [==============================] - 74s 74ms/step - loss: 0.7497 - accuracy: 0.8681 - val_loss: 0.9128 - val_accuracy: 0.7792\n",
            "Epoch 5/30\n",
            "997/997 [==============================] - 74s 74ms/step - loss: 0.7112 - accuracy: 0.8757 - val_loss: 0.8162 - val_accuracy: 0.7986\n",
            "Epoch 6/30\n",
            "997/997 [==============================] - 77s 77ms/step - loss: 0.6611 - accuracy: 0.8828 - val_loss: 0.6665 - val_accuracy: 0.7916\n",
            "Epoch 7/30\n",
            "997/997 [==============================] - 75s 76ms/step - loss: 0.6223 - accuracy: 0.8878 - val_loss: 0.5562 - val_accuracy: 0.8251\n",
            "Epoch 8/30\n",
            "997/997 [==============================] - 74s 74ms/step - loss: 0.5970 - accuracy: 0.8924 - val_loss: 0.5568 - val_accuracy: 0.8347\n",
            "Epoch 9/30\n",
            "997/997 [==============================] - 76s 76ms/step - loss: 0.5766 - accuracy: 0.8958 - val_loss: 0.4054 - val_accuracy: 0.8814\n",
            "Epoch 10/30\n",
            "997/997 [==============================] - 74s 75ms/step - loss: 0.5559 - accuracy: 0.8996 - val_loss: 0.3722 - val_accuracy: 0.8948\n",
            "Epoch 11/30\n",
            "997/997 [==============================] - 85s 85ms/step - loss: 0.5372 - accuracy: 0.9031 - val_loss: 0.3899 - val_accuracy: 0.8904\n",
            "Epoch 12/30\n",
            "997/997 [==============================] - 98s 98ms/step - loss: 0.5249 - accuracy: 0.9053 - val_loss: 0.3853 - val_accuracy: 0.8920\n",
            "Epoch 13/30\n",
            "997/997 [==============================] - 101s 101ms/step - loss: 0.5140 - accuracy: 0.9073 - val_loss: 0.3571 - val_accuracy: 0.8987\n",
            "Epoch 14/30\n",
            "997/997 [==============================] - 102s 102ms/step - loss: 0.5026 - accuracy: 0.9092 - val_loss: 0.3725 - val_accuracy: 0.9021\n",
            "Epoch 15/30\n",
            "997/997 [==============================] - 101s 101ms/step - loss: 0.4919 - accuracy: 0.9109 - val_loss: 0.3802 - val_accuracy: 0.8989\n",
            "Epoch 16/30\n",
            "997/997 [==============================] - 101s 101ms/step - loss: 0.4845 - accuracy: 0.9123 - val_loss: 0.3448 - val_accuracy: 0.9101\n",
            "Epoch 17/30\n",
            "997/997 [==============================] - 101s 101ms/step - loss: 0.4760 - accuracy: 0.9141 - val_loss: 0.2654 - val_accuracy: 0.9302\n",
            "Epoch 18/30\n",
            "997/997 [==============================] - 100s 101ms/step - loss: 0.4689 - accuracy: 0.9153 - val_loss: 0.3583 - val_accuracy: 0.9014\n",
            "Epoch 19/30\n",
            "997/997 [==============================] - 99s 100ms/step - loss: 0.4580 - accuracy: 0.9172 - val_loss: 0.2496 - val_accuracy: 0.9329\n",
            "Epoch 20/30\n",
            "997/997 [==============================] - 94s 94ms/step - loss: 0.4514 - accuracy: 0.9187 - val_loss: 0.2700 - val_accuracy: 0.9311\n",
            "Epoch 21/30\n",
            "997/997 [==============================] - 74s 74ms/step - loss: 0.4520 - accuracy: 0.9187 - val_loss: 0.4111 - val_accuracy: 0.8979\n",
            "Epoch 22/30\n",
            "997/997 [==============================] - 73s 73ms/step - loss: 0.4381 - accuracy: 0.9210 - val_loss: 0.2686 - val_accuracy: 0.9336\n",
            "Epoch 23/30\n",
            "997/997 [==============================] - 74s 74ms/step - loss: 0.4359 - accuracy: 0.9219 - val_loss: 0.3178 - val_accuracy: 0.9222\n",
            "Epoch 24/30\n",
            "997/997 [==============================] - 75s 75ms/step - loss: 0.4321 - accuracy: 0.9227 - val_loss: 0.3448 - val_accuracy: 0.9173\n",
            "Epoch 25/30\n",
            "997/997 [==============================] - 77s 77ms/step - loss: 0.4292 - accuracy: 0.9233 - val_loss: 0.2853 - val_accuracy: 0.9258\n",
            "Epoch 26/30\n",
            "997/997 [==============================] - 76s 77ms/step - loss: 0.4233 - accuracy: 0.9246 - val_loss: 0.2844 - val_accuracy: 0.9288\n",
            "Epoch 27/30\n",
            "997/997 [==============================] - 74s 74ms/step - loss: 0.4236 - accuracy: 0.9246 - val_loss: 0.3743 - val_accuracy: 0.9093\n",
            "Epoch 28/30\n",
            "997/997 [==============================] - 73s 73ms/step - loss: 0.4152 - accuracy: 0.9263 - val_loss: 0.2993 - val_accuracy: 0.9270\n",
            "Epoch 29/30\n",
            "997/997 [==============================] - 73s 73ms/step - loss: 0.4204 - accuracy: 0.9256 - val_loss: 0.3387 - val_accuracy: 0.9072\n",
            "Epoch 30/30\n",
            "997/997 [==============================] - 75s 75ms/step - loss: 0.4184 - accuracy: 0.9258 - val_loss: 0.2812 - val_accuracy: 0.9306\n",
            "Evaluating binary model...\n",
            "Precision: 0.9010661946204346\n",
            "Recall: 0.9060075837417886\n",
            "Threshhold: 0.71\n",
            "         type  label  detected     ratio\n",
            "9   conficker   5996    1111.0  0.185290\n",
            "22       gozi   5500    1414.0  0.257091\n",
            "27     matsnu   5941    2608.0  0.438983\n",
            "36     nymaim   5821      97.0  0.016664\n",
            "40     pushdo   5500    2181.0  0.396545\n",
            "49      simda  21132    8174.0  0.386807\n",
            "50   suppobox   7415     434.0  0.058530\n",
            "51      symmi   9756    3488.0  0.357524\n",
            "56    vawtrak   6220    3603.0  0.579260\n",
            "58      virut   1950      76.0  0.038974\n",
            "            type   label  detected     ratio\n",
            "0     FluBot_dga   82000   78143.0  0.952963\n",
            "3        banjori  461986  426430.0  0.923037\n",
            "10       corebot    5500    5354.0  0.973455\n",
            "11  cryptolocker   43569   38794.0  0.890404\n",
            "12      dircrypt    6054    5036.0  0.831847\n",
            "14        emotet  463549  450677.0  0.972232\n",
            "17        flubot   21997   20912.0  0.950675\n",
            "18        fobber    5784    4299.0  0.743257\n",
            "19      gameover    4000    3981.0  0.995250\n",
            "29       murofet   10281   10021.0  0.974711\n",
            "32        necurs   21717   17150.0  0.789704\n",
            "33        newgoz    1856    1849.0  0.996228\n",
            "38      padcrypt    5668    5402.0  0.953070\n",
            "41        pykspa   37983   30389.0  0.800068\n",
            "42        qadars    7300    6853.0  0.938767\n",
            "43         ramdo    5500    5394.0  0.980727\n",
            "44        ramnit   15632   12723.0  0.813907\n",
            "45       ranbyus   16300   15643.0  0.959693\n",
            "46        rovnix  171995  167863.0  0.975976\n",
            "53         tinba   52322   46068.0  0.880471\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Training binary model...\")\n",
        "model_binary = train_model_binary(X_train, y_train, X_test, y_test)\n",
        "print(\"Evaluating binary model...\")\n",
        "model_eval_binary(model_binary, X_test, y_test, domain_test, type_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training to detect to which of the DGA is belogs"
      ],
      "metadata": {
        "id": "k_tjDG4WHARC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZOzqHqYCBEA",
        "outputId": "1ec320aa-36cc-4a5b-89b5-60e781aa5bb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing data for families model...\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "Training families model...\n",
            "Epoch 1/120\n",
            "165/165 [==============================] - 81s 480ms/step - loss: 0.1201\n",
            "Epoch 2/120\n",
            "165/165 [==============================] - 96s 581ms/step - loss: 0.0336\n",
            "Epoch 3/120\n",
            "165/165 [==============================] - 80s 482ms/step - loss: 0.0249\n",
            "Epoch 4/120\n",
            "165/165 [==============================] - 86s 518ms/step - loss: 0.0190\n",
            "Epoch 5/120\n",
            "165/165 [==============================] - 88s 533ms/step - loss: 0.0160\n",
            "Epoch 6/120\n",
            "165/165 [==============================] - 84s 513ms/step - loss: 0.0149\n",
            "Epoch 7/120\n",
            "165/165 [==============================] - 91s 552ms/step - loss: 0.0117\n",
            "Epoch 8/120\n",
            "165/165 [==============================] - 80s 489ms/step - loss: 0.0118\n",
            "Epoch 9/120\n",
            "165/165 [==============================] - 83s 503ms/step - loss: 0.0100\n",
            "Epoch 10/120\n",
            "165/165 [==============================] - 89s 539ms/step - loss: 0.0078\n",
            "Epoch 11/120\n",
            "165/165 [==============================] - 78s 476ms/step - loss: 0.0080\n",
            "Epoch 12/120\n",
            "165/165 [==============================] - 81s 491ms/step - loss: 0.0071\n",
            "Epoch 13/120\n",
            "165/165 [==============================] - 79s 479ms/step - loss: 0.0065\n",
            "Epoch 14/120\n",
            "165/165 [==============================] - 80s 487ms/step - loss: 0.0050\n",
            "Epoch 15/120\n",
            "165/165 [==============================] - 86s 525ms/step - loss: 0.0055\n",
            "Epoch 16/120\n",
            "165/165 [==============================] - 78s 471ms/step - loss: 0.0055\n",
            "Epoch 17/120\n",
            "165/165 [==============================] - 79s 480ms/step - loss: 0.0061\n",
            "Epoch 18/120\n",
            "165/165 [==============================] - 84s 512ms/step - loss: 0.0049\n",
            "Epoch 19/120\n",
            "165/165 [==============================] - 84s 511ms/step - loss: 0.0048\n",
            "Epoch 20/120\n",
            "165/165 [==============================] - 89s 540ms/step - loss: 0.0045\n",
            "Epoch 21/120\n",
            "165/165 [==============================] - 78s 463ms/step - loss: 0.0043\n",
            "Epoch 22/120\n",
            "165/165 [==============================] - 61s 371ms/step - loss: 0.0046\n",
            "Epoch 23/120\n",
            "165/165 [==============================] - 65s 396ms/step - loss: 0.0047\n",
            "Epoch 24/120\n",
            "165/165 [==============================] - 64s 381ms/step - loss: 0.0056\n",
            "Epoch 25/120\n",
            "165/165 [==============================] - 62s 377ms/step - loss: 0.0049\n",
            "Epoch 26/120\n",
            "165/165 [==============================] - 62s 377ms/step - loss: 0.0051\n",
            "Epoch 27/120\n",
            "165/165 [==============================] - 64s 383ms/step - loss: 0.0044\n",
            "Epoch 28/120\n",
            "165/165 [==============================] - 66s 401ms/step - loss: 0.0038\n",
            "Epoch 29/120\n",
            "165/165 [==============================] - 63s 384ms/step - loss: 0.0043\n",
            "Epoch 30/120\n",
            "165/165 [==============================] - 67s 400ms/step - loss: 0.0037\n",
            "Epoch 31/120\n",
            "165/165 [==============================] - 64s 391ms/step - loss: 0.0041\n",
            "Epoch 32/120\n",
            "165/165 [==============================] - 67s 408ms/step - loss: 0.0041\n",
            "Epoch 33/120\n",
            "165/165 [==============================] - 68s 411ms/step - loss: 0.0042\n",
            "Epoch 34/120\n",
            "165/165 [==============================] - 61s 368ms/step - loss: 0.0039\n",
            "Epoch 35/120\n",
            "165/165 [==============================] - 67s 411ms/step - loss: 0.0042\n",
            "Epoch 36/120\n",
            "165/165 [==============================] - 68s 413ms/step - loss: 0.0043\n",
            "Epoch 37/120\n",
            "165/165 [==============================] - 64s 388ms/step - loss: 0.0041\n",
            "Epoch 38/120\n",
            "165/165 [==============================] - 67s 407ms/step - loss: 0.0040\n",
            "Epoch 39/120\n",
            "165/165 [==============================] - 66s 400ms/step - loss: 0.0036\n",
            "Epoch 40/120\n",
            "165/165 [==============================] - 66s 395ms/step - loss: 0.0037\n",
            "Epoch 41/120\n",
            "165/165 [==============================] - 66s 401ms/step - loss: 0.0043\n",
            "Epoch 42/120\n",
            "165/165 [==============================] - 68s 411ms/step - loss: 0.0032\n",
            "Epoch 43/120\n",
            "165/165 [==============================] - 70s 426ms/step - loss: 0.0034\n",
            "Epoch 44/120\n",
            "165/165 [==============================] - 64s 387ms/step - loss: 0.0036\n",
            "Epoch 45/120\n",
            "165/165 [==============================] - 71s 435ms/step - loss: 0.0034\n",
            "Epoch 46/120\n",
            "165/165 [==============================] - 67s 407ms/step - loss: 0.0033\n",
            "Epoch 47/120\n",
            "165/165 [==============================] - 67s 406ms/step - loss: 0.0034\n",
            "Epoch 48/120\n",
            "165/165 [==============================] - 69s 418ms/step - loss: 0.0029\n",
            "Epoch 49/120\n",
            "165/165 [==============================] - 67s 402ms/step - loss: 0.0035\n",
            "Epoch 50/120\n",
            "165/165 [==============================] - 73s 444ms/step - loss: 0.0036\n",
            "Epoch 51/120\n",
            "165/165 [==============================] - 72s 434ms/step - loss: 0.0031\n",
            "Epoch 52/120\n",
            "165/165 [==============================] - 67s 406ms/step - loss: 0.0033\n",
            "Epoch 53/120\n",
            "165/165 [==============================] - 66s 404ms/step - loss: 0.0028\n",
            "Epoch 54/120\n",
            "165/165 [==============================] - 68s 412ms/step - loss: 0.0028\n",
            "Epoch 55/120\n",
            "165/165 [==============================] - 65s 389ms/step - loss: 0.0026\n",
            "Epoch 56/120\n",
            "165/165 [==============================] - 67s 403ms/step - loss: 0.0028\n",
            "Epoch 57/120\n",
            "165/165 [==============================] - 66s 403ms/step - loss: 0.0027\n",
            "Epoch 58/120\n",
            "165/165 [==============================] - 68s 410ms/step - loss: 0.0028\n",
            "Epoch 59/120\n",
            "165/165 [==============================] - 64s 390ms/step - loss: 0.0026\n",
            "Epoch 60/120\n",
            "165/165 [==============================] - 69s 414ms/step - loss: 0.0027\n",
            "Epoch 61/120\n",
            "165/165 [==============================] - 68s 410ms/step - loss: 0.0031\n",
            "Epoch 62/120\n",
            "165/165 [==============================] - 65s 396ms/step - loss: 0.0025\n",
            "Epoch 63/120\n",
            "165/165 [==============================] - 69s 414ms/step - loss: 0.0026\n",
            "Epoch 64/120\n",
            "165/165 [==============================] - 71s 423ms/step - loss: 0.0025\n",
            "Epoch 65/120\n",
            "165/165 [==============================] - 68s 416ms/step - loss: 0.0023\n",
            "Epoch 66/120\n",
            "165/165 [==============================] - 65s 396ms/step - loss: 0.0025\n",
            "Epoch 67/120\n",
            "165/165 [==============================] - 72s 438ms/step - loss: 0.0027\n",
            "Epoch 68/120\n",
            "165/165 [==============================] - 72s 436ms/step - loss: 0.0026\n",
            "Epoch 69/120\n",
            "165/165 [==============================] - 69s 418ms/step - loss: 0.0027\n",
            "Epoch 70/120\n",
            "165/165 [==============================] - 70s 426ms/step - loss: 0.0024\n",
            "Epoch 71/120\n",
            "165/165 [==============================] - 70s 419ms/step - loss: 0.0023\n",
            "Epoch 72/120\n",
            "165/165 [==============================] - 68s 415ms/step - loss: 0.0026\n",
            "Epoch 73/120\n",
            "165/165 [==============================] - 65s 397ms/step - loss: 0.0025\n",
            "Epoch 74/120\n",
            "165/165 [==============================] - 67s 404ms/step - loss: 0.0027\n",
            "Epoch 75/120\n",
            "165/165 [==============================] - 67s 403ms/step - loss: 0.0026\n",
            "Epoch 76/120\n",
            "165/165 [==============================] - 69s 421ms/step - loss: 0.0023\n",
            "Epoch 77/120\n",
            "165/165 [==============================] - 68s 413ms/step - loss: 0.0021\n",
            "Epoch 78/120\n",
            "165/165 [==============================] - 73s 441ms/step - loss: 0.0027\n",
            "Epoch 79/120\n",
            "165/165 [==============================] - 71s 432ms/step - loss: 0.0023\n",
            "Epoch 80/120\n",
            "165/165 [==============================] - 77s 469ms/step - loss: 0.0024\n",
            "Epoch 81/120\n",
            "165/165 [==============================] - 72s 441ms/step - loss: 0.0024\n",
            "Epoch 82/120\n",
            "165/165 [==============================] - 70s 426ms/step - loss: 0.0026\n",
            "Epoch 83/120\n",
            "165/165 [==============================] - 76s 460ms/step - loss: 0.0026\n",
            "Epoch 84/120\n",
            "165/165 [==============================] - 67s 411ms/step - loss: 0.0023\n",
            "Epoch 85/120\n",
            "165/165 [==============================] - 72s 440ms/step - loss: 0.0022\n",
            "Epoch 86/120\n",
            "165/165 [==============================] - 74s 449ms/step - loss: 0.0022\n",
            "Epoch 87/120\n",
            "165/165 [==============================] - 69s 421ms/step - loss: 0.0021\n",
            "Epoch 88/120\n",
            "165/165 [==============================] - 70s 418ms/step - loss: 0.0021\n",
            "Epoch 89/120\n",
            "165/165 [==============================] - 73s 445ms/step - loss: 0.0021\n",
            "Epoch 90/120\n",
            "165/165 [==============================] - 75s 452ms/step - loss: 0.0023\n",
            "Epoch 91/120\n",
            "165/165 [==============================] - 72s 439ms/step - loss: 0.0024\n",
            "Epoch 92/120\n",
            "165/165 [==============================] - 69s 416ms/step - loss: 0.0025\n",
            "Epoch 93/120\n",
            "165/165 [==============================] - 71s 434ms/step - loss: 0.0025\n",
            "Epoch 94/120\n",
            "165/165 [==============================] - 75s 446ms/step - loss: 0.0022\n",
            "Epoch 95/120\n",
            "165/165 [==============================] - 76s 460ms/step - loss: 0.0021\n",
            "Epoch 96/120\n",
            "165/165 [==============================] - 74s 435ms/step - loss: 0.0021\n",
            "Epoch 97/120\n",
            "165/165 [==============================] - 70s 426ms/step - loss: 0.0022\n",
            "Epoch 98/120\n",
            "165/165 [==============================] - 71s 433ms/step - loss: 0.0020\n",
            "Epoch 99/120\n",
            "165/165 [==============================] - 74s 453ms/step - loss: 0.0022\n",
            "Epoch 100/120\n",
            "165/165 [==============================] - 76s 464ms/step - loss: 0.0021\n",
            "Epoch 101/120\n",
            "165/165 [==============================] - 68s 416ms/step - loss: 0.0023\n",
            "Epoch 102/120\n",
            "165/165 [==============================] - 73s 446ms/step - loss: 0.0021\n",
            "Epoch 103/120\n",
            "165/165 [==============================] - 68s 413ms/step - loss: 0.0022\n",
            "Epoch 104/120\n",
            "165/165 [==============================] - 74s 452ms/step - loss: 0.0021\n",
            "Epoch 105/120\n",
            "165/165 [==============================] - 74s 446ms/step - loss: 0.0022\n",
            "Epoch 106/120\n",
            "165/165 [==============================] - 74s 449ms/step - loss: 0.0022\n",
            "Epoch 107/120\n",
            "165/165 [==============================] - 73s 447ms/step - loss: 0.0021\n",
            "Epoch 108/120\n",
            "165/165 [==============================] - 73s 440ms/step - loss: 0.0020\n",
            "Epoch 109/120\n",
            "165/165 [==============================] - 73s 445ms/step - loss: 0.0021\n",
            "Epoch 110/120\n",
            "165/165 [==============================] - 74s 446ms/step - loss: 0.0020\n",
            "Epoch 111/120\n",
            "165/165 [==============================] - 73s 444ms/step - loss: 0.0019\n",
            "Epoch 112/120\n",
            "165/165 [==============================] - 74s 454ms/step - loss: 0.0019\n",
            "Epoch 113/120\n",
            "165/165 [==============================] - 70s 421ms/step - loss: 0.0019\n",
            "Epoch 114/120\n",
            "165/165 [==============================] - 76s 466ms/step - loss: 0.0019\n",
            "Epoch 115/120\n",
            "165/165 [==============================] - 78s 472ms/step - loss: 0.0020\n",
            "Epoch 116/120\n",
            "165/165 [==============================] - 72s 437ms/step - loss: 0.0020\n",
            "Epoch 117/120\n",
            "165/165 [==============================] - 69s 418ms/step - loss: 0.0019\n",
            "Epoch 118/120\n",
            "165/165 [==============================] - 74s 454ms/step - loss: 0.0019\n",
            "Epoch 119/120\n",
            "165/165 [==============================] - 78s 473ms/step - loss: 0.0016\n",
            "Epoch 120/120\n",
            "165/165 [==============================] - 77s 466ms/step - loss: 0.0017\n"
          ]
        }
      ],
      "source": [
        "print(\"Processing data for families model...\")\n",
        "train_generator, validation_generator, X_data, encoded_labels, steps_per_epoch, domains, labels_type, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff = data_preprocessing_families(df_families)\n",
        "print(\"Training families model...\")\n",
        "model_families = train_model_families(train_generator, steps_per_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the families model"
      ],
      "metadata": {
        "id": "clbkRRkYHtua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "2db5860e920e41d1b3780f8be7e63636",
            "765dcced9b6d48b8b7052f338c8cd046"
          ]
        },
        "id": "iKAX7GVKCBEB",
        "outputId": "05f29b1c-37c9-4493-c505-ca5361d59b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating families model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2db5860e920e41d1b3780f8be7e63636",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Pandas Apply'), FloatProgress(value=0.0, max=1248658.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0.9839387566491385\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "765dcced9b6d48b8b7052f338c8cd046",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Pandas Apply'), FloatProgress(value=0.0, max=647180.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0.05320433044685712\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating families model...\")\n",
        "model_eval_families(model_families, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "Here we show an example of detecting character DGA algorithms with high Precision 0.9 and Recall 0.9 and also high accuracy 0.95 of detecting many character DGA families. In future work we can develop also a word-base DGA model. Our model based on AppShield - BlueField which is a agentless system. By using AppShield we succeeded to detect DGA malwares before the malware connects with his command and control server."
      ],
      "metadata": {
        "id": "Q1S_3hCu6QPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "https://underdefense.com/guides/detecting-dga-domains-machine-learning-approach/"
      ],
      "metadata": {
        "id": "Bf5AZ5Pl7iEm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "dga-appshield-cnn-20220214.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1wxCnfvDjfR",
    "tags": []
   },
   "source": [
    "# DGA Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ViGLx7pDw4a"
   },
   "source": [
    "## Table of Contents\n",
    "* Introduction\n",
    "* Dataset\n",
    "* Data Preprocessing\n",
    "* Binary Model Training and Evaluation\n",
    "* Familieis Model Training and Evaluation\n",
    "* Conclusions\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AnMgLK4D6AL"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Domain generation algorithms (DGA) are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. The large number of potential rendezvous points makes it difficult for law enforcement to effectively shut down botnets, since infected computers will attempt to contact some of these domain names every day to receive updates or commands. The use of public-key cryptography in malware code makes it unfeasible for law enforcement and other actors to mimic commands from the malware controllers as some worms will automatically reject any updates not signed by the malware controllers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wJonJ9RER7S"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We gathered 3M domains from popular datasets of DGAs.\n",
    "i.e. https://data.netlab.360.com/dga/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvA-XuWf79zN",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qtWhJU8N78eq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in /opt/conda/envs/rapids/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tldextract) (3.1.0)\n",
      "Requirement already satisfied: idna in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tldextract) (2.10)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tldextract) (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests>=2.1.0->tldextract) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests>=2.1.0->tldextract) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests>=2.1.0->tldextract) (2021.5.30)\n",
      "Requirement already satisfied: six in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests-file>=1.4->tldextract) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /opt/conda/envs/rapids/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (3.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (21.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (1.48.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (1.21.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from packaging->tensorflow) (2.4.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: dgaintel in /opt/conda/envs/rapids/lib/python3.8/site-packages (2.3)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dgaintel) (2.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dgaintel) (1.21.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (49.6.0.post20210108)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (2.10.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (21.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (1.48.1)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (2.10.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (3.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (1.12.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (14.0.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (0.27.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (2.0.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (2.0.7)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorflow->dgaintel) (1.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow->dgaintel) (0.37.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow->dgaintel) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow->dgaintel) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow->dgaintel) (2.2.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow->dgaintel) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow->dgaintel) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow->dgaintel) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow->dgaintel) (2.0.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->dgaintel) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->dgaintel) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->dgaintel) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow->dgaintel) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->dgaintel) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow->dgaintel) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow->dgaintel) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow->dgaintel) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow->dgaintel) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow->dgaintel) (3.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from packaging->tensorflow->dgaintel) (2.4.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting torch\n",
      "  Downloading torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[K     |███████████████▍                | 372.4 MB 96.3 MB/s eta 0:00:054  |█                               | 21.9 MB 3.0 MB/s eta 0:04:08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████▍   | 689.9 MB 107.6 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 776.3 MB 3.7 kB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/envs/rapids/lib/python3.8/site-packages (from torch) (3.10.0.2)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.12.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: swifter in /opt/conda/envs/rapids/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (1.3.3)\n",
      "Requirement already satisfied: bleach>=3.1.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (4.1.0)\n",
      "Requirement already satisfied: parso>0.4.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (0.7.1)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (4.62.3)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (2.0.0)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (7.6.5)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (2021.9.1)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from swifter) (5.8.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/envs/rapids/lib/python3.8/site-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.8/site-packages (from bleach>=3.1.1->swifter) (21.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from bleach>=3.1.1->swifter) (1.16.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (2021.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (5.4.1)\n",
      "Requirement already satisfied: partd>=0.3.10 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (1.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.18 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (1.21.2)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (7.15.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (3.5.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (1.0.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (5.1.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (5.1.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipywidgets>=7.0.0->swifter) (5.5.5)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (7.0.5)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.1)\n",
      "Requirement already satisfied: pexpect in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (4.4.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (49.6.0.post20210108)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.17.2)\n",
      "Requirement already satisfied: pygments in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (2.10.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (3.0.20)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (4.8.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (21.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from packaging->bleach>=3.1.1->swifter) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from pandas>=1.0.0->swifter) (2021.1)\n",
      "Requirement already satisfied: locket in /opt/conda/envs/rapids/lib/python3.8/site-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/envs/rapids/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (6.4.4)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (20.1.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (22.3.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.11.0)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (6.2.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (3.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (1.8.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.5.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/envs/rapids/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.3)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/envs/rapids/lib/python3.8/site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/rapids/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (2.0.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.8.4)\n",
      "Requirement already satisfied: testpath in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.5.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/envs/rapids/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "! pip install tldextract\n",
    "! pip install tensorflow\n",
    "! pip install dgaintel\n",
    "! pip install torch\n",
    "! pip install swifter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoqOysmTFKo6",
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-FCq782QCBDf"
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tldextract\n",
    "\n",
    "import glob\n",
    "from dgaintel import get_prob\n",
    "import swifter\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dgaintel import get_prob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-QbjcHdFhiG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKI3cdJ7CBDi"
   },
   "outputs": [],
   "source": [
    "# Binary model params\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 1000\n",
    "MALICIOUS_RATIO = 0.01\n",
    "LEARNING_RATE = 0.001\n",
    "# Families model params\n",
    "TRAIN_BATCH_SIZE = 500\n",
    "TEST_BATCH_SIZE = 500\n",
    "EMB_SIZE = 10\n",
    "EPOCHS_SIAMESE = 1#20\n",
    "LEARNING_RATE_SIAMESE = 1e-3\n",
    "CLASS_WEIGHTS = {0: 100, 1:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afOzj7DVFyyb"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4GcBS0clCBDk"
   },
   "outputs": [],
   "source": [
    "# Return URL domain\n",
    "def get_domain(url):\n",
    "    domain = tldextract.extract(url).domain\n",
    "    if domain == 'ddns':\n",
    "        print(url)\n",
    "        urls = url.split('.')\n",
    "        urls_i = urls.index('ddns')\n",
    "        if urls_i == 0:\n",
    "            return 'ddns'\n",
    "        print(urls[urls_i-1])\n",
    "        return urls[urls_i-1]\n",
    "    if domain:\n",
    "        return domain\n",
    "    return ''\n",
    "# Make spaces for character processing\n",
    "def get_domain_space(domain):\n",
    "    try:\n",
    "        return \" \".join(domain)\n",
    "    except:\n",
    "        print(domain)\n",
    "        return \"\"\n",
    "# Split train-test - we are trying change the ratio for small DGA families to test them too \n",
    "def split_train_test_dga(df,ratio=0.8):\n",
    "    df_dga = df[df['label']==1]\n",
    "    df_legit = df[df['label']==0]\n",
    "    X_dga, y_dga = df_dga['domain_1'],df_dga['label']\n",
    "    X_legit, y_legit = df_legit['domain_1'],df_legit['label']\n",
    "    train_dga_i = []\n",
    "    train_ben_i = []\n",
    "    test_dga_i = []\n",
    "    test_ben_i = []\n",
    "    # Make the dga train set to be more equale between families without dominant family\n",
    "    for fam in pd.unique(df_dga['type']):\n",
    "        df_dga_fam = df_dga[df_dga['type']==fam]\n",
    "        # Shuffle the dataframe rows\n",
    "        df_dga_fam = df_dga_fam.sample(frac = 1)\n",
    "        if len(df_dga_fam)>10000:\n",
    "            train_dga_i.extend(df_dga_fam.iloc[0:int(ratio*10000)].index)\n",
    "            test_dga_i.extend(df_dga_fam.iloc[int(ratio*10000):].index)\n",
    "        else:\n",
    "            train_dga_i.extend(df_dga_fam.iloc[0:int(ratio*len(df_dga_fam))].index)\n",
    "            test_dga_i.extend(df_dga_fam.iloc[int(ratio*len(df_dga_fam)):].index)\n",
    "    df_legit = df_legit.sample(frac = 1)\n",
    "    train_ben_i.extend(df_legit.iloc[0:int(ratio*len(df_legit))].index)\n",
    "    test_ben_i.extend(df_legit.iloc[int(ratio*len(df_legit)):].index)\n",
    "    train_dga_i.extend(train_ben_i)\n",
    "    test_dga_i.extend(test_ben_i)\n",
    "    X_train = df['domain_1'][train_dga_i]\n",
    "    y_train = df['label'][train_dga_i]\n",
    "    X_test = df['domain_1'][test_dga_i]\n",
    "    y_test = df['label'][test_dga_i]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "# Create data loader for the neural-net\n",
    "def create_data_loader(data,label):\n",
    "    tensor_data = torch.Tensor(data.astype(int))\n",
    "    tensor_label = torch.Tensor(label)\n",
    "    my_dataset = TensorDataset(tensor_data,tensor_label)\n",
    "    data_loader = DataLoader(my_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    return data_loader\n",
    "# Create batches for Siamese networks\n",
    "def create_batch_offline(indices,batch_size,anc_indices,domain_indices=None):\n",
    "    \"\"\"choose an anchor, a positive and a negative batch.\n",
    "    if domain_indices is given, choose anchor only from the specified domains indices. \"\"\"\n",
    "    x_anchors = np.zeros((batch_size, 75))\n",
    "    x_positives = np.zeros_like(x_anchors)\n",
    "    x_negatives = np.zeros_like(x_anchors)\n",
    "\n",
    "    y = encoded_labels[indices]\n",
    "    anc_indices = np.intersect1d(anc_indices,domain_indices,assume_unique=True)\n",
    "    for i in range(0, batch_size):\n",
    "        anc_idx = np.random.choice(anc_indices) \n",
    "        x_anchor = X_data[anc_idx]\n",
    "        y_anchor = encoded_labels[anc_idx]\n",
    "\n",
    "        indices_for_pos = indices[np.where(y == y_anchor)]  #resulting array alway >=1 (the anchor itself)\n",
    "        pos_idx = np.random.choice(indices_for_pos)\n",
    "        indices_for_neg = indices[np.where(y != y_anchor)] \n",
    "        neg_idx = np.random.choice(indices_for_neg)\n",
    "\n",
    "        x_positive = X_data[pos_idx]\n",
    "        x_negative = X_data[neg_idx] \n",
    "        \n",
    "        x_anchors[i] = x_anchor\n",
    "        x_positives[i] = x_positive\n",
    "        x_negatives[i] = x_negative\n",
    "        \n",
    "    return [x_anchors, x_positives, x_negatives] \n",
    "# Generation of Triplets for Triplet-loss\n",
    "def triplets_generator(**kwargs):\n",
    "    while True:\n",
    "        x = create_batch_offline(**kwargs)\n",
    "        dummy_y = np.zeros((x[0].shape[0], 3 , EMB_SIZE))  #dummy y (never used) the size of the siamese input is required\n",
    "        yield x,dummy_y\n",
    "# Triplet loss\n",
    "alpha = 1.5  #value between 0-2\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor, positive, negative = y_pred[:,:EMB_SIZE], y_pred[:,EMB_SIZE:2*EMB_SIZE], y_pred[:,2*EMB_SIZE:]\n",
    "    p_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "    n_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "    return tf.reduce_mean(tf.maximum(0., p_dist - n_dist + alpha))        \n",
    "# Calculate the distance from centroid        \n",
    "def argmin_label(row, ref, ref_save):\n",
    "    emb = np.array(row[[0,1,2,3,4,5,6,7,8,9]])\n",
    "    list_dist = [np.sum(np.power(emb-ref_save[key],2)) for key in ref]\n",
    "    arg_m = np.argmin(list_dist)\n",
    "    dist_m = np.min(list_dist)\n",
    "    row['predict_label'] = list(domains)[arg_m]\n",
    "    row['predict_dist'] = dist_m\n",
    "    return row\n",
    "# Processing the binary data\n",
    "def data_preprocessing_binary(df):\n",
    "    # Make spaces between domain chars\n",
    "    df['domain_1'] = df['domain'].apply(get_domain_space)\n",
    "    # Split train-test\n",
    "    X_train, X_test, y_train, y_test = split_train_test_dga(df,0.8)\n",
    "\n",
    "    domain_test = df['domain'].iloc[X_test.index]\n",
    "    type_test = df['type'].iloc[X_test.index]\n",
    "    # Convert text to tokens\n",
    "    X_train_np = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train_np = pad_sequences(X_train_np, maxlen=75, padding='post')\n",
    "    X_test_np = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test_np = pad_sequences(X_test_np, maxlen=75, padding='post')\n",
    "    \n",
    "    X_train = np.array(X_train_np).astype(int)\n",
    "    X_test = np.array(X_test_np).astype(int)\n",
    "    \n",
    "    train_loader = create_data_loader(X_train, list(y_train))\n",
    "    test_loader = create_data_loader(X_test, list(y_test))\n",
    "            \n",
    "    return X_train, y_train, X_test, y_test, domain_test, type_test\n",
    "# Train the binary model\n",
    "def train_model_binary(X_train, y_train, X_test, y_test):\n",
    "    # Defining the model\n",
    "    inputA = tf.keras.layers.Input(shape=(X_train.shape[1],), name='input')\n",
    "    x = tf.keras.layers.Embedding(max_features, 128, input_length=75)(inputA)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "    x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "    model = tf.keras.Model(inputs=x.input, outputs=x.output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #batch_size = 1000\n",
    "    model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "             validation_data=([X_test], y_test), class_weight=CLASS_WEIGHTS)\n",
    "    return model\n",
    "# Evaluate the binary model\n",
    "def model_eval_binary(model, X_test, y_test, domain_test, type_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    D_test = pd.DataFrame()\n",
    "    \n",
    "    D_test[\"domain\"] = domain_test\n",
    "    D_test[\"type\"] = type_test\n",
    "    D_test[\"label\"] = y_test\n",
    "    D_test[\"pred\"] = y_pred\n",
    "    \n",
    "    recall = []\n",
    "    precision = []\n",
    "    ratio_malicious_benign = 0.01\n",
    "    flag_pass = False\n",
    "    thr_final = 0\n",
    "    for thr in np.arange(0, 1, 0.01):\n",
    "        FPs = len(D_test[(D_test['pred']>thr) & (D_test['label']==0)])\n",
    "        len_ben = len(D_test[D_test['label']==0])\n",
    "        len_mal = len(D_test[D_test['label']==0])*ratio_malicious_benign\n",
    "        recall_step = len(D_test[(D_test['pred']>thr) & (D_test['label']==1)])/len(D_test[D_test['label']==1])\n",
    "        recall.append(recall_step)\n",
    "        TPs = len_mal*recall_step\n",
    "        precision.append(TPs/(TPs+FPs))\n",
    "        if TPs/(TPs+FPs) > 0.9 and flag_pass == False:\n",
    "            print('Precision: {}'.format(TPs/(TPs+FPs)))\n",
    "            print('Recall: {}'.format(recall_step))\n",
    "            print('Threshhold: {}'.format(thr))\n",
    "            thr_final = thr\n",
    "            flag_pass = True\n",
    "    pyplot.plot(recall, precision, marker='.', label='CNN Pytorch')\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    pyplot.title('DGA model')\n",
    "       \n",
    "    D_test_mal = pd.DataFrame(D_test[D_test['label']==1].groupby(['type'], as_index=False)['label'].sum())\n",
    "    D_test_mal_detected = pd.DataFrame(D_test[(D_test['label']==1) & (D_test['pred']>thr_final)].groupby(['type'], as_index=False)['label'].sum())\n",
    "    D_test_mal_detected.columns = ['type','detected']\n",
    "    D_test_mal = pd.merge(D_test_mal, D_test_mal_detected,how = \"left\", on=[\"type\"])\n",
    "    D_test_mal['detected'] = D_test_mal['detected'].fillna(0)\n",
    "    D_test_mal['ratio'] = D_test_mal['detected']/D_test_mal['label']\n",
    "    print(D_test_mal[(D_test_mal['ratio']<thr_final) & (D_test_mal['label']>D_test_mal['label'].median())])\n",
    "    print(D_test_mal[(D_test_mal['ratio']>thr_final) & (D_test_mal['label']>D_test_mal['label'].median())])\n",
    "# Processing the families data\n",
    "def data_preprocessing_families(df):    \n",
    "    # Merge dgas families with the same pattern\n",
    "    df['type'] = df['type'].replace('FluBot_dga','flubot')\n",
    "    df['type'] = df['type'].replace('fobber_v2','fobber')\n",
    "    df['type'] = df['type'].replace('legit','alexa')\n",
    "    df['type'] = df['type'].replace('pykspa_v2_real','pykspa')\n",
    "    df['type'] = df['type'].replace('pykspa_v2_fake','pykspa')\n",
    "    df['type'] = df['type'].replace('gameoverdga','gameover')\n",
    "    # Merge others dgas families to 1 label\n",
    "    for type_dga in pd.unique(df['type']):\n",
    "        if type_dga not in['goz','bazarbackdoor','bamital','gspy','dyre','enviserv','chinad','monerodownloader','emotet','ramdo','padcrypt','qadars','banjori','corebot','rovnix','flubot','gameover','alexa']:\n",
    "            df['type'] = df['type'].replace(type_dga,'alexa')\n",
    "    \n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    labels_type = df['type']\n",
    "    # Label encoding the dga families\n",
    "    le = LabelEncoder()\n",
    "    encoded_labels = le.fit_transform(labels_type)\n",
    "    dict_type_count = df['type'].value_counts().to_dict()\n",
    "    dict_type_count.pop('alexa')\n",
    "    df['domain_1'] = df['domain'].apply(get_domain_space)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_train_test_dga(df,0.8)\n",
    "       \n",
    "    # Convert text to tokens\n",
    "    X_data = tokenizer.texts_to_sequences(df['domain_1'])\n",
    "    X_data = pad_sequences(X_data, maxlen=75, padding='post')\n",
    "    domains = dict_type_count.keys()\n",
    "    df['new_col'] = df['type'].isin(domains).astype(int)\n",
    "    domains_idx = np.array(df.index[df['new_col'] == 1])\n",
    "    noise_idx = np.array(df.index[df['new_col'] == 0])\n",
    "    indices = domains_idx\n",
    "    train_indices_same = np.intersect1d(X_train.index, domains_idx, assume_unique=False)\n",
    "    train_indices_diff = np.intersect1d(X_train.index, noise_idx, assume_unique=False)\n",
    "    test_indices_same = np.intersect1d(X_test.index, domains_idx, assume_unique=False)\n",
    "    test_indices_diff = np.intersect1d(X_test.index, noise_idx, assume_unique=False) \n",
    "    train_classes, train_cnt = np.unique(encoded_labels[train_indices_same], return_counts=True)\n",
    "    test_classes, test_cnt = np.unique(encoded_labels[test_indices_same], return_counts=True)\n",
    "    stacked = np.stack((train_cnt,test_cnt),axis=1)\n",
    "    \n",
    "    anc_idx = np.random.choice(train_indices_same) \n",
    "    anchor = X_data[anc_idx]\n",
    "    encoded_labels_train = encoded_labels[domains_idx]\n",
    "    steps_per_epoch = int(train_indices_same.size/TRAIN_BATCH_SIZE)\n",
    "    validation_steps = int(test_indices_same.size/TEST_BATCH_SIZE)\n",
    "\n",
    "    train_generator = triplets_generator(indices=X_train.index,batch_size=TRAIN_BATCH_SIZE,anc_indices=train_indices_same,domain_indices=train_indices_same)\n",
    "    validation_generator = triplets_generator(indices=X_test.index,batch_size=TEST_BATCH_SIZE,anc_indices=test_indices_same,domain_indices=test_indices_same)\n",
    "    \n",
    "    return train_generator, validation_generator, X_data, encoded_labels, steps_per_epoch, domains, labels_type, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff\n",
    "# Train the families model\n",
    "def train_model_families(train_generator, steps_per_epoch):  \n",
    "    # Defining the model\n",
    "    inputA = tf.keras.layers.Input(shape=(75,), name='input')\n",
    "    x = tf.keras.layers.Embedding(max_features, 128, input_length=75)(inputA)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=4, activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(EMB_SIZE, activation=None)(x)\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x,axis=1), name='output')(x)\n",
    "    x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "    model = tf.keras.Model(inputs=x.input, outputs=x.output)\n",
    "    \n",
    "    input_anchor = tf.keras.layers.Input(shape=(75))\n",
    "    input_positive = tf.keras.layers.Input(shape=(75))\n",
    "    input_negative = tf.keras.layers.Input(shape=(75))\n",
    "\n",
    "    embedding_anchor = model(input_anchor)\n",
    "    embedding_positive = model(input_positive)\n",
    "    embedding_negative = model(input_negative)\n",
    "\n",
    "    output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)\n",
    "\n",
    "    siamese_net = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)\n",
    "    \n",
    "    siamese_net.compile(loss=triplet_loss, optimizer=Adam(learning_rate=LEARNING_RATE_SIAMESE))\n",
    "    \n",
    "    history = siamese_net.fit(\n",
    "    train_generator, steps_per_epoch=steps_per_epoch, epochs=EPOCHS_SIAMESE, workers=8 ,use_multiprocessing=True)\n",
    "    \n",
    "    return model\n",
    "# Evaluate families model\n",
    "def model_eval_families(model, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff):\n",
    "    train_indices_same = np.sort(train_indices_same)\n",
    "    train_indices_diff = np.sort(train_indices_diff)\n",
    "    x_train_same = X_data[train_indices_same]\n",
    "    x_train_diff = X_data[train_indices_diff]\n",
    "    y_train_same = labels_type.iloc[train_indices_same]\n",
    "    y_train_diff = labels_type.iloc[train_indices_diff]\n",
    "    x_test_same = X_data[test_indices_same]\n",
    "    x_test_diff = X_data[test_indices_diff]\n",
    "    y_test_same = labels_type.iloc[test_indices_same]\n",
    "    y_test_diff = labels_type.iloc[test_indices_diff]\n",
    "        \n",
    "    x_train_same_emb = model.predict(x_train_same)\n",
    "    x_train_diff_emb = model.predict(x_train_diff)\n",
    "    \n",
    "    x_test_same_emb = model.predict(x_test_same)\n",
    "    x_test_diff_emb = model.predict(x_test_diff)\n",
    "    \n",
    "    # Create a dict of the output model\n",
    "    #create a dict of vector embeddings per class:\n",
    "    ref ={}\n",
    "    for domain in domains:\n",
    "        x_domain = x_train_same[np.where(y_train_same == domain)[0]]\n",
    "        ref[domain] = model(x_domain)\n",
    "    ref_save = {}\n",
    "    # Create dict of anchors\n",
    "    for key in ref:\n",
    "        ref_save[key] = ref[key][0]\n",
    "    ref_save_df = pd.DataFrame()\n",
    "    ref_save_df['Family'] = ref_save.keys()\n",
    "    for i in range(len(ref_save['emotet'])):\n",
    "        list_vec = []\n",
    "        for key in ref_save:\n",
    "            list_vec.append(ref_save[key][i].numpy())\n",
    "        ref_save_df[i] = list_vec\n",
    "    \n",
    "    y_test_same_list = y_test_same.tolist()\n",
    "    df_test_same_emb = pd.DataFrame(x_test_same_emb)\n",
    "    df_test_same_emb['label'] = y_test_same_list\n",
    "    df_test_same_emb_mini = df_test_same_emb\n",
    "    df_test_same_emb_mini = df_test_same_emb_mini.swifter.apply(lambda row : argmin_label(row, ref, ref_save),axis=1)\n",
    "    \n",
    "    print(len(df_test_same_emb_mini[(df_test_same_emb_mini['predict_label']==df_test_same_emb_mini['label']) & (df_test_same_emb_mini['predict_dist']<0.5)])/len(df_test_same_emb_mini))\n",
    "    \n",
    "    y_test_diff_list = y_test_diff.tolist()\n",
    "    df_test_diff_emb = pd.DataFrame(x_test_diff_emb)\n",
    "    df_test_diff_emb['label'] = y_test_diff_list\n",
    "    df_test_diff_emb_mini = df_test_diff_emb\n",
    "    df_test_diff_emb_mini = df_test_diff_emb_mini.swifter.apply(lambda row : argmin_label(row, ref, ref_save),axis=1)\n",
    "    \n",
    "    print(len(df_test_diff_emb_mini[(df_test_diff_emb_mini['predict_dist']<0.5) & ~df_test_diff_emb_mini['predict_label'].isin(['simda','fobber','pykspa_v1'])])/len(df_test_same_emb_mini[(~df_test_same_emb_mini['label'].isin(['simda','fobber','pykspa_v1'])) & (df_test_same_emb_mini['predict_label']==df_test_same_emb_mini['label']) & (df_test_same_emb_mini['predict_dist']<0.5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaOrtbm-F42x"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NF4MTVA0CBD2",
    "outputId": "c57736bb-420c-4456-d7a7-d31dd587b233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tokenizer...\n",
      "Reading data for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.word_index = pd.read_csv('../models/tokenizer.csv').set_index('keys')['values'].to_dict()\n",
    "max_features = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Reading tokenizer...\")\n",
    "# Read tokenizer\n",
    "print(\"Reading data for training\")\n",
    "df_binary = pd.read_csv(\"../datasets/dga_training_dataset.csv\")\n",
    "df_families = df_binary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9DuHUfhGOX6"
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5IYjShJLCBD7",
    "outputId": "2c98434e-725a-4f91-fb2c-5566cf8b93b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for binary model...\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing data for binary model...\")\n",
    "X_train, y_train, X_test, y_test, domain_test, type_test = data_preprocessing_binary(df_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3s7-d2DG1JA"
   },
   "source": [
    "# Binary model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "M0_v2MkkCBD9",
    "outputId": "94fb6425-fa87-440b-f1b6-6d9540fca79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training binary model...\n",
      "Evaluating binary model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training binary model...\")\n",
    "model_binary = train_model_binary(X_train, y_train, X_test, y_test)\n",
    "print(\"Evaluating binary model...\")\n",
    "model_eval_binary(model_binary, X_test, y_test, domain_test, type_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_tjDG4WHARC"
   },
   "source": [
    "# Training to detect to which of the DGA is belogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "IZOzqHqYCBEA",
    "outputId": "1ec320aa-36cc-4a5b-89b5-60e781aa5bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for families model...\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing data for families model...\")\n",
    "train_generator, validation_generator, X_data, encoded_labels, steps_per_epoch, domains, labels_type, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff = data_preprocessing_families(df_families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training families model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training families model...\")\n",
    "model_families = train_model_families(train_generator, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clbkRRkYHtua",
    "tags": []
   },
   "source": [
    "# Evaluation of the families model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "2db5860e920e41d1b3780f8be7e63636",
      "765dcced9b6d48b8b7052f338c8cd046"
     ]
    },
    "id": "iKAX7GVKCBEB",
    "outputId": "05f29b1c-37c9-4493-c505-ca5361d59b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating families model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating families model...\")\n",
    "model_eval_families(model_families, train_indices_same, train_indices_diff, test_indices_same, test_indices_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1S_3hCu6QPl"
   },
   "source": [
    "# Conclusions\n",
    "Here we show an example of detecting character DGA algorithms with high Precision 0.9 and Recall 0.9 and also high accuracy 0.95 of detecting many character DGA families. In future work we can develop also a word-base DGA model. Our model based on AppShield - BlueField which is a agentless system. By using AppShield we succeeded to detect DGA malwares before the malware connects with the command and control server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bf5AZ5Pl7iEm"
   },
   "source": [
    "# References\n",
    "- https://data.netlab.360.com/dga/\n",
    "- https://underdefense.com/guides/detecting-dga-domains-machine-learning-approach/\n",
    "- https://developer.nvidia.com/networking/doca \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dga-appshield-cnn-20220214.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
